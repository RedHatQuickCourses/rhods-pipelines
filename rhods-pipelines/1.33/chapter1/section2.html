<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Elyra Pipelines :: Automation using Data Science Pipelines</title>
    <link rel="prev" href="section1.html">
    <link rel="next" href="section3.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Automation using Data Science Pipelines</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/rhods-pipelines/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header><div class="body">
<div class="nav-container" data-component="rhods-pipelines" data-version="1.33">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Automation using Data Science Pipelines</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">Chapter 1</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section1.html">Section 1</a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="section2.html">Elyra Pipelines</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section3.html">Kubeflow SDK</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../chapter2/index.html">Chapter 3</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../appendix/appendix.html">Appendix A</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Automation using Data Science Pipelines</span>
    <span class="version">1.33</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Automation using Data Science Pipelines</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1.33</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Automation using Data Science Pipelines</a></li>
    <li><a href="index.html">Chapter 1</a></li>
    <li><a href="section2.html">Elyra Pipelines</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Elyra Pipelines</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Elyra provides a visual pipeline editor for building pipelines from Python and R scripts as well as Jupyter notebooks, simplifying the conversion of multiple files into batch jobs or workflows. A pipeline in Elyra consists of nodes that are connected with each other to define execution dependencies.</p>
</div>
<div class="paragraph">
<p>Elyra&#8217;s visual pipeline editor lets you assemble pipelines by dragging and dropping supported files onto the canvas and defining their dependencies. After you&#8217;ve assembled the pipeline and are ready to run it, the editor takes care of generating the Tekton YAML definition on the fly and submitting it to the Data Science Pipelines backend.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_creating_a_data_science_pipeline_with_elyra"><a class="anchor" href="#_creating_a_data_science_pipeline_with_elyra"></a>Creating a Data Science Pipeline with Elyra</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In order to create Elyra pipelines with the visual pipeline editor:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Launch JupyterLab with the Elyra extension installed.</p>
</li>
<li>
<p>Create a new pipeline by clicking on the Elyra <code>Pipeline Editor</code> icon.</p>
</li>
<li>
<p>Add each node to the pipeline by dragging and dropping notebooks or scripts from the file browser onto the pipeline editor canvas.</p>
</li>
<li>
<p>Connect the nodes to define the flow of execution.</p>
</li>
<li>
<p>Configure each node by right-clicking on it, clicking 'Open Properties', and setting the appropriate runtime image and file dependencies.</p>
</li>
<li>
<p>You can also inject environment variables, secrets, and define output files.</p>
</li>
<li>
<p>Once the pipeline is complete, you can submit it to the Data Science Pipelines engine.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_elyra_runtime_configuration"><a class="anchor" href="#_elyra_runtime_configuration"></a>Elyra runtime configuration</h2>
<div class="sectionbody">
<div class="paragraph">
<p>A runtime configuration provides Elyra access to the Data Science Pipelines backend for scalable pipeline execution. You can manage runtime configurations using the JupyterLab UI or the Elyra CLI. The runtime configuration is included and is pre-configured for submitting pipelines to Data Science Pipelines. Refer to the <a href="https://elyra.readthedocs.io/en/latest/user_guide/runtime-conf.html#kubeflow-pipelines-configuration-settings">Elyra documentation</a> for more information about Elyra and the available runtime configuration options.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_exercise_offline_scoring_for_fraud_detection"><a class="anchor" href="#_exercise_offline_scoring_for_fraud_detection"></a>Exercise: offline scoring for fraud detection</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_setup"><a class="anchor" href="#_setup"></a>Setup</h3>
<div class="paragraph">
<p>Let&#8217;s walk through an example of creating and running an Elyra pipeline for fraud detection. We&#8217;ll first set up the prerequisites to get started with Elyra and Data Science Pipelines.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Data Science Pipelines has two major dependencies: OpenShift Pipelines and S3 object storage. The setup of OpenShift Pipelines and Minio for S3 has already been covered earlier in this course, so we won&#8217;t discuss it here.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="_set_up_data_science_pipelines"><a class="anchor" href="#_set_up_data_science_pipelines"></a>Set up Data Science Pipelines</h4>
<div class="paragraph">
<p>Data Science Pipelines relies on a backend S3 object storage for persisting artifacts, logs, and outputs that are associated with individual pipeline runs.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Any S3-compliant storage solution can be used for Data Science Pipelines, including AWS S3, OpenShift Data Foundation, or Minio. In this course we will use Minio as a particularly lightweight S3 storage solution. Red Hat recommends OpenShift Data Foundation in scenarios where data resilience and desaster recovery are important concerns.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Create a new data science project called <code>fraud-detection</code>. Enter the project.</p>
</div>
<div class="paragraph">
<p>Under <code>Pipelines</code>, select <code>Create a pipeline server</code>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/create_pipeline_server.png" alt="create pipeline server">
</div>
</div>
<div class="paragraph">
<p>Under <code>Object storage connection</code>, select <code>Create new data connection</code>.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Enter the following details:</p>
</li>
<li>
<p>Name: <code>fraud-detection-pipelines</code></p>
</li>
<li>
<p>Access key: <code>minio</code></p>
</li>
<li>
<p>Secret key: <code>minio123</code></p>
</li>
<li>
<p>Endpoint: <code><a href="http://minio-service.minio.svc:9000" class="bare">http://minio-service.minio.svc:9000</a></code></p>
</li>
<li>
<p>Bucket: <code>fraud-detection-pipelines</code></p>
</li>
</ul>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/configure_pipeline_server.png" alt="configure pipeline server">
</div>
</div>
<div class="paragraph">
<p>Hit <code>Configure</code>.</p>
</div>
</div>
<div class="sect3">
<h4 id="_prepare_the_bucket"><a class="anchor" href="#_prepare_the_bucket"></a>Prepare the bucket</h4>
<div class="paragraph">
<p>Log into the Minio console.</p>
</div>
<div class="paragraph">
<p>Create a new S3 bucket named <code>fraud-detection</code> in Minio.</p>
</div>
<div class="paragraph">
<p>Download the <code>model-latest.onnx</code> file from the <a href="https://github.com/mamurak/os-mlops-artefacts/tree/fraud-detection-model-v0.1/models/fraud-detection">model artefact repository</a> and upload it to the S3 bucket.</p>
</div>
<div class="paragraph">
<p>Download the <code>live-data.csv</code> file from the <a href="https://github.com/mamurak/os-mlops-artefacts/tree/fraud-detection-data-v0.1/data/fraud-detection">data set repository</a> and upload it to the S3 bucket.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/fraud-detection-bucket.png" alt="fraud detection bucket">
</div>
</div>
</div>
<div class="sect3">
<h4 id="_import_the_custom_workbench_image"><a class="anchor" href="#_import_the_custom_workbench_image"></a>Import the custom workbench image</h4>
<div class="paragraph">
<p>Return to the RHOAI dashboard.</p>
</div>
<div class="paragraph">
<p>Under <code>Settings</code> select <code>Notebook images</code>.</p>
</div>
<div class="paragraph">
<p>Select <code>Import new image</code>.</p>
</div>
<div class="paragraph">
<p>Enter the following details:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Repository: <code>quay.io/mmurakam/workbenches:fraud-detection-v1.0.1</code></p>
</li>
<li>
<p>Name: <code>Fraud detection workbench</code></p>
</li>
<li>
<p>Optionally a description.</p>
</li>
</ul>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/import-workbench-image.png" alt="import workbench image">
</div>
</div>
<div class="paragraph">
<p>Hit <code>Import</code>.</p>
</div>
</div>
<div class="sect3">
<h4 id="_set_up_the_workbench"><a class="anchor" href="#_set_up_the_workbench"></a>Set up the workbench</h4>
<div class="paragraph">
<p>In the fraud-detection data science project, create a new data workbench and enter the following details:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Name: <code>fraud-detection-workbench</code></p>
</li>
<li>
<p>Notebook image: <code>Fraud detection workbench</code></p>
</li>
<li>
<p>Container size: <code>Small</code></p>
</li>
<li>
<p>Create a new persistent storage with name <code>fraud-detection</code> and size 5 GB.</p>
</li>
<li>
<p>Use a data connection &#8594; Create new data connection, enter the connection parameters for consuming the <code>fraud-detection</code> bucket.</p>
</li>
</ul>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/create-workbench.png" alt="create workbench">
</div>
</div>
<div class="paragraph">
<p>Hit <code>Create workbench</code>.</p>
</div>
</div>
<div class="sect3">
<h4 id="_set_up_pipeline_storage"><a class="anchor" href="#_set_up_pipeline_storage"></a>Set up pipeline storage</h4>
<div class="paragraph">
<p>We&#8217;ll now create a persistent volume that the pipeline will use to persist and exchange data across tasks.</p>
</div>
<div class="paragraph">
<p>Select <code>Add cluster storage</code>. Give it the name <code>offline-scoring-data-volume</code> and set <code>Persistent storage size</code> to 5 GB.</p>
</div>
<div class="paragraph">
<p>Hit <code>Add storage</code>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/pipeline-storage.png" alt="pipeline storage">
</div>
</div>
<div class="paragraph">
<p>Finally, once the fraud detection workbench is up and running, access it.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_working_with_elyra"><a class="anchor" href="#_working_with_elyra"></a>Working with Elyra</h3>
<div class="sect3">
<h4 id="_the_code"><a class="anchor" href="#_the_code"></a>The code</h4>
<div class="paragraph">
<p>Within the workbench, clone the course git repository:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">https://github.com/RedHatQuickCourses/rhods-qc-apps.git</code></pre>
</div>
</div>
<div class="paragraph">
<p>Within the cloned repository, navigate to the <code>5.pipelines/elyra</code> folder. The folder contains all the code that is needed for running offline scoring with a given model. In particular, it contains the Python modules:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>data_ingestion.py</code> for downloading a dataset from an S3 bucket,</p>
</li>
<li>
<p><code>preprocessing.py</code> for preprocessing the downloaded dataset,</p>
</li>
<li>
<p><code>model_loading.py</code> for downloading a model artefact from an S3 bucket,</p>
</li>
<li>
<p><code>scoring.py</code> for running the classification on the preprocessed data using the downloaded model,</p>
</li>
<li>
<p><code>results_upload.py</code> for uploading the classification results to an S3 bucket.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>In Elyra, each pipeline step is implemented by a separate file such as Python modules in our example. In line with software development best practices, pipelines are best implemented in a modular fashion, i.e. across several components. This way, generic pipeline tasks like data ingestion can be re-used in many different pipelines addressing different use cases.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Explore these Python modules to get an understanding of the workflow. A few points of note:</p>
</div>
<div class="paragraph">
<p>Three tasks (data ingestion, model loading, results upload) access the S3 backend. Instead of hardcoding the connection parameters into the pipeline code, these parameters are instead read from the environment at runtime:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">s3_endpoint_url = environ.get('AWS_S3_ENDPOINT')
s3_access_key = environ.get('AWS_ACCESS_KEY_ID')
s3_secret_key = environ.get('AWS_SECRET_ACCESS_KEY')
s3_bucket_name = environ.get('AWS_S3_BUCKET')</code></pre>
</div>
</div>
<div class="paragraph">
<p>This approach is in line with best practices of handling credentials and allows us to control which S3 buckets are consumed in a given runtime context without changing the code. Importantly, these parameters are stored in a data connection, which is mounted into workbenches and pipeline pods to expose their values to the pipeline tasks.</p>
</div>
<div class="paragraph">
<p>Three tasks (preprocessing, scoring, results upload) require access to files that were stored by previous tasks. This is not an issue if we execute the code within the same filesystem like in the workbench, but since each task is later executed within a separate container in Data Science Pipelines, we can&#8217;t assume that the tasks automatically have access to each other&#8217;s files. Note that the dataset and result files are stored and read within a given data folder (<code>/data</code>), while the model artefact is stored and read in the respective working directory. We will see later how Elyra is capable of handling data passing in these contexts.</p>
</div>
</div>
<div class="sect3">
<h4 id="_running_the_code_interactively"><a class="anchor" href="#_running_the_code_interactively"></a>Running the code interactively</h4>
<div class="paragraph">
<p>The Python modules cover the offline scoring tasks end-to-end, so we can run the code in the workbench to perform all needed tasks interactively.</p>
</div>
<div class="paragraph">
<p>For this, open the <code>offline-scoring.ipynb</code> Jupyter notebook. This notebook references each of the Python modules, so once you execute the notebook cells, you&#8217;re executing the individual tasks implemented in the modules. This is a great way to develop, test, and debug the code that the pipeline will execute.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>It&#8217;s not recommended to rely on workbenches and Jupyter notebooks for production use cases. Implement your pipeline code in native Python modules and test it interactively in a notebook session. Applying the code in production requires stability, auditability, and reproducibility, which workbenches and Jupyter notebooks are not designed for.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="_building_the_pipeline"><a class="anchor" href="#_building_the_pipeline"></a>Building the pipeline</h4>
<div class="paragraph">
<p>Let&#8217;s now use Elyra to package the code into a pipeline and submit it to the Data Science Pipelines backend in order to:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>rely on the pipeline scheduler to manage the pipeline execution without having to depend on my workbench session,</p>
</li>
<li>
<p>keep track of the pipeline execution along with the previous executions,</p>
</li>
<li>
<p>be able to control resource usage of individual pipeline tasks in a fine-grained manner.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Within the workbench, open the launcher by clicking on the blue plus button.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/launcher.png" alt="launcher">
</div>
</div>
<div class="paragraph">
<p>Click on the <code>Pipeline Editor</code> tile in the launcher menu. This opens up Elyra&#8217;s visual pipeline editor. Use the visual pipeline editor to drag-and-drop files from the file browser onto the canvas area. These files then define the individual tasks of your pipeline.</p>
</div>
<div class="paragraph">
<p>The pipeline should start by ingesting the dataset that we want to classify, so drag the <code>data_ingestion.py</code> module onto the empty canvas.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/pipeline-1.png" alt="pipeline 1">
</div>
</div>
<div class="paragraph">
<p>Next, the ingested data should be preprocessed, so drag the <code>preprocessing.py</code> module onto the canvas, right next to the <code>data_ingestion.py</code> module.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/pipeline-2.png" alt="pipeline 2">
</div>
</div>
<div class="paragraph">
<p>We have now defined two tasks of the pipeline, but the order of processing is not defined yet. To instruct Elyra to start with data ingestion and perform preprocessing only after data ingestion has finished, connect the <code>Output Port</code> (right black dot of the task icon) of the <code>data_ingestion</code> task with the <code>Input Port</code> (left black dot of the task icon) of the <code>preprocessing</code> task by drawing a line between these ports (click, hold &amp; draw, release).</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/pipeline-3.png" alt="pipeline 3">
</div>
</div>
<div class="paragraph">
<p>You should now see the two nodes connected through a solid line. We have now defined a simple pipeline with two tasks, which are executed sequentially, first data ingestion and then preprocessing.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>By visually defining pipeline tasks and connections, we can define <em>graphs</em> spanning many nodes and interconnections. Elyra and Data Science Pipelines support the creation and execution of arbitrary <em>directed acyclic graphs</em> (DAGs), i.e. graphs with a sequential order of nodes and without loops.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Now add the <code>scoring.py</code> and <code>results_upload.py</code> modules to the pipeline and connect them to form a straight 4-step pipeline.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/pipeline-4.png" alt="pipeline 4">
</div>
</div>
<div class="paragraph">
<p>It looks like we have captured the end-to-end process of offline scoring from data ingestion to scoring and results upload, but there is one step missing. A closer look at the <code>scoring.py</code> module reveals that it requires not only the preprocessed data from the previous <code>preprocessing.py</code> step but also the model file, which is created by the <code>model_loading.py</code> module. So we have to ensure that model loading is executed before scoring. However, since model loading does not depend on data ingestion and preprocessing, we can have these tasks executed in parallel. To do this, add the <code>model_loading.py</code> onto the canvas and connect its <code>Output Port</code> with the <code>Input Port</code> of <code>scoring.py</code>.</p>
</div>
<div class="paragraph">
<p><span class="image"><img src="_images/pipeline-5.png" alt="pipeline 5"></span></p>
</div>
<div class="paragraph">
<p>We have now created the final graph representation of the offline scoring pipeline using the five available modules. With this we have fully defined the full pipeline code and its order of execution.</p>
</div>
</div>
<div class="sect3">
<h4 id="_configuring_the_pipeline"><a class="anchor" href="#_configuring_the_pipeline"></a>Configuring the pipeline</h4>
<div class="paragraph">
<p>Before we can submit our pipeline, we have to configure the pipeline to specify:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>the set of dependencies for each step, i.e. the corresponding runtime images,</p>
</li>
<li>
<p>how data is passed between the steps,</p>
</li>
<li>
<p>how the S3 credentials are exposed as environment variables during runtime,</p>
</li>
<li>
<p>and, optionally, the available compute resources per step.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Corresponding to our custom workbench image, there is a custom pipeline runtime image containing the same libraries, which we will use throughout the pipeline tasks.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>For every custom workbench image, we recommend building a corresponding pipeline runtime image to ensure consistency between interactive and pipeline-based code execution.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Before we can use a custom runtime image in our pipeline, we have to add it to the Elyra runtime image library. To do this, open the <code>Runtime Images</code> menu from the left toolbar.</p>
</div>
<div class="paragraph">
<p>Select <code>Create new runtime image</code> via the plus sign in the top portion of the menu.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/runtime-images.png" alt="runtime images">
</div>
</div>
<div class="paragraph">
<p>Fill out the required values:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>display name: <code>fraud detection runtime</code></p>
</li>
<li>
<p>image name: <code>quay.io/mmurakam/runtimes:fraud-detection-v0.2.0</code></p>
</li>
</ul>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/runtime-image-2.png" alt="runtime image 2">
</div>
</div>
<div class="paragraph">
<p>Hit <code>Save &amp; Close</code>.</p>
</div>
<div class="paragraph">
<p>Let&#8217;s now set this runtime image in our pipeline. To do this, open the pipeline settings in the Elyra pipeline editor via <code>Open Panel</code> in the top right corner of the editor.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/pipeline-config-1.png" alt="pipeline config 1">
</div>
</div>
<div class="paragraph">
<p>You can now see the <code>PIPELINE PROPERTIES</code> tab of the settings menu. Here you can configure the default settings that are applied on all pipeline tasks.</p>
</div>
<div class="paragraph">
<p>Scroll down to <code>Generic Node Defaults</code> and click on the drop down menu of <code>Runtime Image</code>. Select the <code>fraud detection runtime</code> that we just defined.</p>
</div>
<div class="paragraph">
<p>We can use the <code>Kubernetes Secrets</code> setting to expose our data connection parameters to the pipeline tasks as environment variables. For each of the data connection parameters, add an entry to <code>Kubernetes Secrets</code> with the following values:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>Environment Variable</code>: the parameter name,</p>
</li>
<li>
<p><code>Secret Name</code>: <code>aws-connection-fraud-detection</code> (the name of the Kubernetes secret belonging to the data connection),</p>
</li>
<li>
<p><code>Secret Key</code>: the parameter name.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Ensure there is an entry for each of the parameters:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>AWS_ACCESS_KEY_ID</code></p>
</li>
<li>
<p><code>AWS_SECRET_ACCESS_KEY</code></p>
</li>
<li>
<p><code>AWS_S3_ENDPOINT</code></p>
</li>
<li>
<p><code>AWS_S3_BUCKET</code></p>
</li>
</ul>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/pipeline-config-3.png" alt="pipeline config 3">
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The AWS default region is another parameter in the data connection, which is used for AWS S3-based connections. In case of self-managed S3 backends such as Minio or OpenShift Data Foundation, this parameter can be safely ignored.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>For simplicity, we have just defined a single runtime image and set of environment variables that are reused in each pipeline task. To optimize pipeline task behavior and control access to credentials, we recommend configuring these settings on a node basis.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Let&#8217;s now set up data passing between the pipeline steps, in particular the model artifact and the dataset.</p>
</div>
<div class="paragraph">
<p>Click on the <code>model_loading.py</code> node. If you&#8217;re still in the configuration menu, you should now see the <code>NODE PROPERTIES</code> tab. If not, right-click on the node and select <code>Open Properties</code>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/pipeline-config-4.png" alt="pipeline config 4">
</div>
</div>
<div class="paragraph">
<p>You can now view and edit the node-specific settings for the selected node. Under <code>Runtime Image</code> and <code>Kubernetes Secret</code>, you can see that the global pipeline settings are used by default.</p>
</div>
<div class="paragraph">
<p>In the <code>Outputs</code> section, you can declare one or more <em>output files</em>. These output files are created by this pipeline task and are made available to all subsequent tasks.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>By default, all files within a containerized task are removed after its execution, so declaring files explicitly as output files is one way to ensure that they can be reused in downstream tasks.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p><code>Add</code> the file <code>model.onnx</code> as an output file. This ensures that the downloaded model artifact is available to the <code>scoring.py</code> task.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/pipeline-config-5.png" alt="pipeline config 5">
</div>
</div>
<div class="paragraph">
<p>Let&#8217;s now enable passing the dataset and results between the pipeline tasks. For this, open the node properties of the <code>data_ingestion.py</code> node. Instead of declaring the downloaded data file as an output file, we&#8217;ll pass it through a mounted volume.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Output files are uploaded by Elyra to the corresponding S3 bucket of the respective pipeline run. This is useful for tracking the intermediate results and artifacts produced within the pipeline.</p>
</div>
<div class="paragraph">
<p>If a task generates a large number of volume of data that need to be passed, network traffic and storage capacity for the intermediate results may become an issue. In this case, it is recommended to pass the data through mounted volumes, which prevents sending these files to the S3 backend.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Scroll down to <code>Data Volumes</code> and select <code>Add</code>.</p>
</div>
<div class="paragraph">
<p>In the <code>Mount Path</code> field enter <code>/data</code>, which is where our pipeline tasks are storing and reading the data and results files.</p>
</div>
<div class="paragraph">
<p>In the <code>Persistent Volume Claim Name</code> field enter <code>offline-scoring-data-volume</code>, which refers to the cluster storage that we created earlier for this pipeline.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/pipeline-config-6.png" alt="pipeline config 6">
</div>
</div>
<div class="paragraph">
<p>Add this data volume entry to the nodes <code>preprocessing.py</code>, <code>scoring.py</code>, and <code>results_upload.py</code>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>We could have declared the data volume as a global pipeline property for simplicity. However, this would have prevented parallel execution of model loading and data ingestion/preprocessing since data volumes can only be used by single tasks by default.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Finally, rename the pipeline file to <code>offline-scoring.pipeline</code> and hit <code>Save Pipeline</code> in the top toolbar.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/pipeline-config-7.png" alt="pipeline config 7">
</div>
</div>
</div>
<div class="sect3">
<h4 id="_running_the_pipeline"><a class="anchor" href="#_running_the_pipeline"></a>Running the pipeline</h4>
<div class="paragraph">
<p>We have now fully created and configured the pipeline, so let&#8217;s now see it in action!</p>
</div>
<div class="paragraph">
<p>In the visual editor, click on the Play icon (<code>Run Pipeline</code>). Leave the default values and hit <code>OK</code>.</p>
</div>
<div class="paragraph">
<p>Elyra is now converting your pipeline definition into a Tekton YAML representation and sending it to the Data Science Pipelines backend. After a few second you should see confirmation that the pipeline has been successfully submitted.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/pipeline-submit.png" alt="pipeline submit">
</div>
</div>
<div class="paragraph">
<p>To monitor the pipeline&#8217;s execution, you can click on the <code>Run Details</code> link, which takes you to the pipeline run view within the RHOAI dashboard. Here you can track in real-time how each pipeline task is processed and whether it fails or resolves successfully.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/pipeline-run.png" alt="pipeline run">
</div>
</div>
<div class="paragraph">
<p>To confirm that the pipeline has indeed produced fraud detection scoring results, view the content of the <code>fraud-detection</code> bucket. You should now see a new CSV file containing the predicted result of each transaction within the used dataset.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/fraud-detection-bucket-2.png" alt="fraud detection bucket 2">
</div>
</div>
<div class="paragraph">
<p>If you head back to the <code>Runs</code> overview in the RHOAI dashboard, you can see the history of all ongoing and previous pipeline executions and compare their run durations and status.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/pipeline-runs.png" alt="pipeline runs">
</div>
</div>
<div class="paragraph">
<p>In the <code>Scheduled</code> tab you&#8217;re able to schedule runs of the offline scoring pipeline according to a predefined schedule such as daily or according to a Cron statement.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/pipeline-scheduled.png" alt="pipeline scheduled">
</div>
</div>
</div>
<div class="sect3">
<h4 id="_tracking_the_pipeline_artifacts"><a class="anchor" href="#_tracking_the_pipeline_artifacts"></a>Tracking the pipeline artifacts</h4>
<div class="paragraph">
<p>Let&#8217;s finally peek behind the scenes and inspect the S3 bucket that Elyra and Data Science Pipelines use to store the pipeline artifacts.</p>
</div>
<div class="paragraph">
<p>View the contents of the <code>fraud-detection-pipelines</code> bucket, which we referenced through the <code>pipelines</code> data connection. You can see three types of folders:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>pipelines</code>: A folder used by Data Science Pipelines to store all pipeline definitions in Tekton YAML format.</p>
</li>
<li>
<p><code>artifacts</code>: A folder used by Data Science Pipelines to store the metadata of each pipeline task for each pipeline run.</p>
</li>
<li>
<p>One folder for each pipeline run with name <code>[pipeline-name]-[timestamp]</code>. These folders are managed by Elyra and contain all file dependencies, log files, and output files of each task.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If you need to troubleshoot a pipeline, the log files within the Elyra bucket are the best place to check any code-related issues. Other places are the Tekton run and pod events logs, which reveal integration or infrastructure-related problems.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/pipelines-bucket.png" alt="pipelines bucket">
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/pipeline-artifacts.png" alt="pipeline artifacts">
</div>
</div>
<div class="paragraph">
<p>Now that we have seen how to work with Data Science Pipelines through Elyra, let&#8217;s take a closer look at the Kubeflow Pipelines SDK.</p>
</div>
</div>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="section1.html">Section 1</a></span>
  <span class="next"><a href="section3.html">Kubeflow SDK</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
