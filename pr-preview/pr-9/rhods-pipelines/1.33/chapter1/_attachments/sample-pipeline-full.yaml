apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: edge-pipeline
  annotations:
    tekton.dev/output_artifacts: '{"load-trigger-data": [{"key": "artifacts/$PIPELINERUN/load-trigger-data/Output.tgz",
      "name": "load-trigger-data-Output", "path": "/tmp/outputs/Output/data"}], "prep-data-train-model":
      [{"key": "artifacts/$PIPELINERUN/prep-data-train-model/parameter_data.tgz",
      "name": "prep-data-train-model-parameter_data", "path": "/tmp/outputs/parameter_data/data"}],
      "prep-data-train-model-2": [{"key": "artifacts/$PIPELINERUN/prep-data-train-model-2/parameter_data.tgz",
      "name": "prep-data-train-model-2-parameter_data", "path": "/tmp/outputs/parameter_data/data"}]}'
    tekton.dev/input_artifacts: '{"model-inference": [{"name": "load-trigger-data-Output",
      "parent_task": "load-trigger-data"}, {"name": "prep-data-train-model-2-parameter_data",
      "parent_task": "prep-data-train-model-2"}], "model-upload-notify": [{"name":
      "load-trigger-data-Output", "parent_task": "load-trigger-data"}, {"name": "prep-data-train-model-parameter_data",
      "parent_task": "prep-data-train-model"}], "prep-data-train-model": [{"name":
      "load-trigger-data-Output", "parent_task": "load-trigger-data"}], "prep-data-train-model-2":
      [{"name": "load-trigger-data-Output", "parent_task": "load-trigger-data"}]}'
    tekton.dev/artifact_bucket: mlpipeline
    tekton.dev/artifact_endpoint: minio-service.kubeflow:9000
    tekton.dev/artifact_endpoint_scheme: http://
    tekton.dev/artifact_items: '{"load-trigger-data": [["Output", "$(results.Output.path)"]],
      "model-inference": [], "model-upload-notify": [], "prep-data-train-model": [["parameter_data",
      "$(workspaces.prep-data-train-model.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/parameter_data"]],
      "prep-data-train-model-2": [["parameter_data", "$(workspaces.prep-data-train-model-2.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/parameter_data"]]}'
    sidecar.istio.io/inject: "false"
    tekton.dev/track_artifact: "true"
    tekton.dev/template: ''
    pipelines.kubeflow.org/big_data_passing_format: $(workspaces.$TASK_NAME.path)/artifacts/$ORIG_PR_NAME/$TASKRUN_NAME/$TASK_PARAM_NAME
    pipelines.kubeflow.org/pipeline_spec: '{"description": "edge pipeline demo", "inputs":
      [{"name": "file_obj", "type": "String"}, {"name": "src_bucket", "type": "String"},
      {"default": "412356", "name": "VIN", "optional": true}, {"default": "270", "name":
      "epoch_count", "optional": true, "type": "Integer"}], "name": "edge-pipeline"}'
  labels:
    pipelines.kubeflow.org/pipelinename: ''
    pipelines.kubeflow.org/generation: ''
spec:
  params:
  - name: VIN
    value: '412356'
  - name: epoch_count
    value: '270'
  - name: file_obj
    value: ''
  - name: src_bucket
    value: ''
  pipelineSpec:
    params:
    - name: VIN
      default: '412356'
    - name: epoch_count
      default: '270'
    - name: file_obj
    - name: src_bucket
    tasks:
    - name: load-trigger-data
      params:
      - name: file_obj
        value: $(params.file_obj)
      - name: src_bucket
        value: $(params.src_bucket)
      taskSpec:
        steps:
        - name: main
          args:
          - --data-file
          - $(inputs.params.file_obj)
          - --bucket-details
          - $(inputs.params.src_bucket)
          - --file-destination
          - /opt/data/pitstop/data/unibo-powertools-dataset/unibo-powertools-dataset/
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def load_trigger_data(data_file,bucket_details,file_destination):
                '''load data file passed from cloud event into relevant location'''
                import boto3
                import os
                import logging
                import zipfile
                import time

                experiment = "lstm_autoencoder_rul_unibo_powertools"
                experiment_name = time.strftime("%Y-%m-%d-%H-%M-%S") + '_' + experiment

                endpoint_url = os.environ["s3_host"]
                aws_access_key_id = os.environ["s3_access_key"]
                aws_secret_access_key = os.environ["s3_secret_access_key"]
                logging.info("S3 creds %s %s %s ", endpoint_url, aws_access_key_id, aws_secret_access_key)
                logging.info("Trigger data bucket %s file %s ", bucket_details, data_file)

                s3_target = boto3.resource(
                    's3',
                    endpoint_url=endpoint_url,
                    aws_access_key_id=aws_access_key_id,
                    aws_secret_access_key=aws_secret_access_key,
                    aws_session_token=None,
                    config=boto3.session.Config(signature_version='s3v4'),
                    verify=False
                )

                with open('/tmp/'+data_file, 'wb') as f:
                    s3_target.meta.client.download_fileobj(bucket_details, data_file, f)

                with zipfile.ZipFile('/tmp/'+data_file, 'r') as zip_ref:
                    zip_ref.extractall(file_destination)

                os.listdir(file_destination)
                return experiment_name

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import argparse
            _parser = argparse.ArgumentParser(prog='Load trigger data', description='load data file passed from cloud event into relevant location')
            _parser.add_argument("--data-file", dest="data_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--bucket-details", dest="bucket_details", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--file-destination", dest="file_destination", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = load_trigger_data(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          env:
          - name: s3_host
            value: http://rook-ceph-rgw-ceph-object-store.openshift-storage.svc:8080
          - name: s3_access_key
            valueFrom:
              secretKeyRef:
                key: AWS_ACCESS_KEY_ID
                name: s3-secret
          - name: s3_secret_access_key
            valueFrom:
              secretKeyRef:
                key: AWS_SECRET_ACCESS_KEY
                name: s3-secret
          image: registry.access.redhat.com/ubi8/python-38
          volumeMounts:
          - mountPath: /opt/data/pitstop/
            name: batterydatavol
        params:
        - name: file_obj
        - name: src_bucket
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: batterydatavol
          persistentVolumeClaim:
            claimName: batterydata
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Load trigger
              data", "outputs": [{"name": "Output", "type": "String"}], "version":
              "Load trigger data@sha256=fb0deae8293ee1973c9018bbc4cb00d7ceb1194a570e581fe1ae38e96adc0330"}'
    - name: prep-data-train-model
      params:
      - name: epoch_count
        value: $(params.epoch_count)
      - name: load-trigger-data-Output
        value: $(tasks.load-trigger-data.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --data-path
          - /opt/data/pitstop/
          - --epoch-count
          - $(inputs.params.epoch_count)
          - --experiment-name
          - $(inputs.params.load-trigger-data-Output)
          - --run-mode
          - '0'
          - --parameter-data
          - $(workspaces.prep-data-train-model.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/parameter_data
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n\
            \    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return\
            \ file_path\n\ndef prep_data_train_model(data_path, epoch_count, parameter_data,\
            \ experiment_name, run_mode=0):\n    \"\"\"Preps the data for processing\"\
            \"\"\n    import numpy as np\n    import pandas as pd\n    import sys\n\
            \    import pickle\n    import logging\n    from importlib import reload\n\
            \    from tensorflow import keras\n    from keras import layers, regularizers\n\
            \    from keras.models import Model\n    from keras import backend as\
            \ K\n    from keras.models import Sequential, Model\n    from keras.layers\
            \ import Dense, Dropout, Activation, TimeDistributed, Input, Concatenate\n\
            \    from keras.optimizers import Adam\n    from keras.layers import LSTM,\
            \ Masking\n    from data_processing.unibo_powertools_data import UniboPowertoolsData,\
            \ CycleCols\n    from data_processing.model_data_handler import ModelDataHandler\n\
            \    from data_processing.prepare_rul_data import RulHandler\n    sys.path.append(data_path)\n\
            \    reload(logging)\n    logging.basicConfig(format='%(asctime)s [%(levelname)s]:\
            \ %(message)s', level=logging.INFO, datefmt='%Y/%m/%d %H:%M:%S')\n   \
            \ #help=\"0 normal Training (default), 1 Bad Training, 2 Inference\",\
            \ default=0)\n\n    # Normal Training\n    if run_mode == 0:\n       \
            \ logging.info(\"Normal Training\")\n        train_names = [ ]\n     \
            \   test_names = [ ]\n        # epoch_count=270\n\n    elif run_mode ==\
            \ 1: # POOR TRAINING\n        logging.info(\"Poor Training\")\n      \
            \  train_names = [ ]\n\n        test_names = [ ]\n        # epoch_count=40\n\
            \    else: #INFERENCING\n        logging.info(\"Inferencing\")\n     \
            \   train_names = [ ]\n        test_names = [ ]\n\n    # # Load Data\n\
            \n    dataset = UniboPowertoolsData(\n        test_types=[],\n       \
            \ chunk_size=1000000,\n        lines=[37, 40],\n        charge_line=37,\n\
            \        discharge_line=40,\n        base_path=data_path\n    )\n\n  \
            \  ################################################NOC\n    dataset.prepare_data(train_names,\
            \ test_names)\n    dataset_handler = ModelDataHandler(dataset, [\n   \
            \     CycleCols.VOLTAGE,\n        CycleCols.CURRENT,\n        CycleCols.TEMPERATURE\n\
            \    ])\n\n    rul_handler = RulHandler()\n\n    # # Data preparation\n\
            \n    capacity_tresholds = { }\n\n    (train_x, train_y_soh, test_x, test_y_soh,\n\
            \    train_battery_range, test_battery_range,\n    time_train, time_test,\
            \ current_train, current_test) = dataset_handler.get_discharge_whole_cycle_future(train_names,\
            \ test_names, min_cycle_length=300)\n\n    train_y = rul_handler.prepare_y_future(train_names,\
            \ train_battery_range, train_y_soh, current_train, time_train, capacity_tresholds)\n\
            \    test_y = rul_handler.prepare_y_future(test_names, test_battery_range,\
            \ test_y_soh, current_test, time_test, capacity_tresholds)\n    x_norm\
            \ = rul_handler.Normalization()\n    x_norm.fit(train_x)\n\n    train_x\
            \ = x_norm.normalize(train_x)\n    test_x = x_norm.normalize(test_x)\n\
            \n    AUTOENCODER_WEIGHTS = '2023-02-09-15-50-22_autoencoder_gl_unibo_powertools'\n\
            \    N_CYCLE = 500\n    WARMUP_TRAIN = 15\n    WARMUP_TEST = 30\n\n  \
            \  opt = keras.optimizers.Adam(learning_rate=0.0002)\n    LATENT_DIM =\
            \ 10\n\n    class Autoencoder(Model):\n        def __init__(self, latent_dim):\n\
            \            super(Autoencoder, self).__init__()\n            self.latent_dim\
            \ = latent_dim\n\n            encoder_inputs = layers.Input(shape=(train_x.shape[1],\
            \ train_x.shape[2]))\n            encoder_conv1 = layers.Conv1D(filters=8,\
            \ kernel_size=10, strides=2, activation='relu', padding='same')(encoder_inputs)\n\
            \            encoder_pool1 = layers.MaxPooling1D(5, padding='same')(encoder_conv1)\n\
            \            encoder_conv2 = layers.Conv1D(filters=8, kernel_size=4, strides=1,\
            \ activation='relu', padding='same')(encoder_pool1)\n            encoder_pool2\
            \ = layers.MaxPooling1D(3, padding='same')(encoder_conv2)\n          \
            \  encoder_flat1 = layers.Flatten()(encoder_pool1)\n            encoder_flat2\
            \ = layers.Flatten()(encoder_pool2)\n            encoder_concat = layers.concatenate([encoder_flat1,\
            \ encoder_flat2])\n            encoder_outputs = layers.Dense(self.latent_dim,\
            \ activation='relu')(encoder_concat)\n            self.encoder = Model(inputs=encoder_inputs,\
            \ outputs=encoder_outputs)\n\n            decoder_inputs = layers.Input(shape=(self.latent_dim,))\n\
            \            decoder_dense1 = layers.Dense(10*8, activation='relu')(decoder_inputs)\n\
            \            decoder_reshape1 = layers.Reshape((10, 8))(decoder_dense1)\n\
            \            decoder_upsample1 = layers.UpSampling1D(3)(decoder_reshape1)\n\
            \            decoder_convT1 = layers.Conv1DTranspose(filters=8, kernel_size=4,\
            \ strides=1, activation='relu', padding='same')(decoder_upsample1)\n \
            \           decoder_upsample2 = layers.UpSampling1D(5)(decoder_convT1)\n\
            \            decoder_convT2 = layers.Conv1DTranspose(filters=8, kernel_size=10,\
            \ strides=2, activation='relu', padding='same')(decoder_upsample2)\n \
            \           decoder_outputs = layers.Conv1D(3, kernel_size=3, activation='relu',\
            \ padding='same')(decoder_convT2)\n            self.decoder = Model(inputs=decoder_inputs,\
            \ outputs=decoder_outputs)\n\n        def call(self, x):\n           \
            \ encoded = self.encoder(x)\n            decoded = self.decoder(encoded)\n\
            \            return decoded\n\n    autoencoder = Autoencoder(LATENT_DIM)\n\
            \n    autoencoder.compile(optimizer=opt, loss='mse', metrics=['mse', 'mae',\
            \ 'mape', keras.metrics.RootMeanSquaredError(name='rmse')])\n    autoencoder.encoder.summary()\n\
            \    autoencoder.decoder.summary()\n    autoencoder.load_weights(data_path\
            \ + 'data/results/trained_model/%s/model' % AUTOENCODER_WEIGHTS)\n   \
            \ # compression\n    train_x = autoencoder.encoder(train_x).numpy()\n\
            \    test_x = autoencoder.encoder(test_x).numpy()\n    logging.info(\"\
            compressed train x shape {}\".format(train_x.shape))\n    logging.info(\"\
            compressed test x shape {}\".format(test_x.shape))\n    test_x = test_x[:,~np.all(train_x\
            \ == 0, axis=0)]#we need same column number of training\n    train_x =\
            \ train_x[:,~np.all(train_x == 0, axis=0)]\n    logging.info(\"compressed\
            \ train x shape without zero column {}\".format(train_x.shape))\n    logging.info(\"\
            compressed test x shape without zero column {}\".format(test_x.shape))\n\
            \n    x_norm = rul_handler.Normalization()\n    x_norm.fit(train_x)\n\
            \    train_x = x_norm.normalize(train_x)\n    test_x = x_norm.normalize(test_x)\n\
            \    train_x = rul_handler.battery_life_to_time_series(train_x, N_CYCLE,\
            \ train_battery_range)\n    test_x = rul_handler.battery_life_to_time_series(test_x,\
            \ N_CYCLE, test_battery_range)\n    train_x, train_y, train_battery_range,\
            \ train_y_soh = rul_handler.delete_initial(train_x, train_y, train_battery_range,\
            \ train_y_soh, WARMUP_TRAIN)\n    test_x, test_y, test_battery_range,\
            \ test_y_soh = rul_handler.delete_initial(test_x, test_y, test_battery_range,\
            \ test_y_soh, WARMUP_TEST)\n\n    # first one is SOH, we keep only RUL\n\
            \    train_y = train_y[:,1]\n    test_y = test_y[:,1]\n\n    # # Y normalization\n\
            \    y_norm = rul_handler.Normalization()\n    y_norm.fit(train_y)\n \
            \   train_y = y_norm.normalize(train_y)\n    test_y = y_norm.normalize(test_y)\
            \  \n\n    ## Only when training\n    if run_mode != 2:\n        opt =\
            \ keras.optimizers.Adam(lr=0.000003)\n        model = Sequential()\n \
            \       model.add(Masking(input_shape=(train_x.shape[1], train_x.shape[2])))\n\
            \        model.add(LSTM(128, activation='tanh',\n                    return_sequences=True,\n\
            \                    kernel_regularizer=regularizers.l2(0.0002)))\n  \
            \      model.add(LSTM(64, activation='tanh', return_sequences=False,\n\
            \                    kernel_regularizer=regularizers.l2(0.0002)))\n  \
            \      model.add(Dense(64, activation='selu', kernel_regularizer=regularizers.l2(0.0002)))\n\
            \        model.add(Dense(32, activation='selu', kernel_regularizer=regularizers.l2(0.0002)))\n\
            \        model.add(Dense(1, activation='linear'))\n        model.summary()\n\
            \n        model.compile(optimizer=opt, loss='huber', metrics=['mse', 'mae',\
            \ 'mape', keras.metrics.RootMeanSquaredError(name='rmse')])\n\n      \
            \  history = model.fit(train_x, train_y, \n                          \
            \      epochs=epoch_count, \n                                batch_size=32,\n\
            \                                verbose=2,\n                        \
            \        validation_split=0.1\n                            )\n       \
            \ model_path= data_path+'data/results/trained_model/%s.h5' % experiment_name\n\
            \n        model.save(model_path)\n        logging.info(\"Model saved to\
            \ %s\",model_path)\n\n        hist_df = pd.DataFrame(history.history)\n\
            \        hist_csv_file = data_path+'data/results/trained_model/%s_history.csv'\
            \ % experiment_name\n        with open(hist_csv_file, mode='w') as f:\n\
            \            hist_df.to_csv(f)\n\n    data_store = [train_x, train_y,\
            \ train_battery_range, train_y_soh, y_norm, test_x, test_y]\n\n    with\
            \ open(parameter_data, \"b+w\") as f:\n        pickle.dump(data_store,f)\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Prep data train\
            \ model', description='Preps the data for processing')\n_parser.add_argument(\"\
            --data-path\", dest=\"data_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--epoch-count\", dest=\"epoch_count\", type=int,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--experiment-name\"\
            , dest=\"experiment_name\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--run-mode\", dest=\"run_mode\", type=int, required=False,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--parameter-data\"\
            , dest=\"parameter_data\", type=_make_parent_dirs_and_return_path, required=True,\
            \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
            \n_outputs = prep_data_train_model(**_parsed_args)\n"
          image: registry.access.redhat.com/ubi8/python-38
          volumeMounts:
          - mountPath: /opt/data/pitstop
            name: batterydatavol
          env:
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
        - image: busybox
          name: output-taskrun-name
          command:
          - sh
          - -ec
          - echo -n "$(context.taskRun.name)" > "$(results.taskrun-name.path)"
        - image: busybox
          name: copy-results-artifacts
          command:
          - sh
          - -ec
          - |
            set -exo pipefail
            TOTAL_SIZE=0
            copy_artifact() {
            if [ -d "$1" ]; then
              tar -czvf "$1".tar.gz "$1"
              SUFFIX=".tar.gz"
            fi
            ARTIFACT_SIZE=`wc -c "$1"${SUFFIX} | awk '{print $1}'`
            TOTAL_SIZE=$( expr $TOTAL_SIZE + $ARTIFACT_SIZE)
            touch "$2"
            if [[ $TOTAL_SIZE -lt 3072 ]]; then
              if [ -d "$1" ]; then
                tar -tzf "$1".tar.gz > "$2"
              elif ! awk "/[^[:print:]]/{f=1} END{exit !f}" "$1"; then
                cp "$1" "$2"
              fi
            fi
            }
            copy_artifact $(workspaces.prep-data-train-model.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/parameter_data $(results.parameter-data.path)
          onError: continue
          env:
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
        params:
        - name: epoch_count
        - name: load-trigger-data-Output
        results:
        - name: parameter-data
          type: string
          description: /tmp/outputs/parameter_data/data
        - name: taskrun-name
          type: string
        volumes:
        - name: batterydatavol
          persistentVolumeClaim:
            claimName: batterydata
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Prep data train
              model", "outputs": [{"name": "parameter_data"}], "version": "Prep data
              train model@sha256=fc6806ccee5930fde6985ee26a7dcc4a9e55262b39679fcd827a438e955fba54"}'
        workspaces:
        - name: prep-data-train-model
      runAfter:
      - load-trigger-data
      workspaces:
      - name: prep-data-train-model
        workspace: edge-pipeline
    - name: model-upload-notify
      params:
      - name: load-trigger-data-Output
        value: $(tasks.load-trigger-data.results.Output)
      - name: prep-data-train-model-trname
        value: $(tasks.prep-data-train-model.results.taskrun-name)
      taskSpec:
        steps:
        - name: main
          args:
          - --data-path
          - /opt/data/pitstop/
          - --paramater-data
          - $(workspaces.model-upload-notify.path)/artifacts/$ORIG_PR_NAME/$(params.prep-data-train-model-trname)/parameter_data
          - --experiment-name
          - $(inputs.params.load-trigger-data-Output)
          - --model-bucket
          - battery-model-bucket
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'kaleido' 'paho-mqtt' 'boto3' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
            -m pip install --quiet --no-warn-script-location 'kaleido' 'paho-mqtt'
            'boto3' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def model_upload_notify(data_path,paramater_data,experiment_name,model_bucket=\"\
            battery-model-bucket\"):\n    \"\"\"Upload model and notify\"\"\"\n  \
            \  import os  \n    import logging\n    from importlib import reload\n\
            \    import pickle\n    from datetime import datetime\n    import numpy\
            \ as np\n    import pandas as pd\n    import plotly.graph_objects as go\n\
            \    import json\n    import random\n    import boto3\n    import zipfile\
            \ \n    import tensorflow as tf\n    from paho.mqtt import client as mqtt_client\n\
            \    from tensorflow import keras\n    from keras import layers, regularizers\n\
            \    from keras import backend as K\n    from keras.models import Sequential,\
            \ Model\n    from keras.layers import Dense, Dropout, Activation, TimeDistributed,\
            \ Input, Concatenate\n    from keras.optimizers import Adam\n    from\
            \ keras.layers import LSTM, Masking\n\n    logging.basicConfig(format='%(asctime)s\
            \ [%(levelname)s]: %(message)s', level=logging.DEBUG, datefmt='%Y/%m/%d\
            \ %H:%M:%S')\n\n    broker_cert=os.getenv(\"mqtt_cert\",\"/opt/certs/public.cert\"\
            )\n    broker=os.getenv(\"mqtt_broker\",\"not set\")\n    port=os.getenv(\"\
            mqtt_port\",\"-1\")\n    topic=\"edgedev/modelupdate\"\n    logging.info(\"\
            MQTT params Broker=%s Port=%s Topic=%s\",broker,port,topic)\n\n    f =\
            \ open(paramater_data,\"b+r\")\n    data_store = pickle.load(f)\n    train_x=data_store[0]\n\
            \    train_y=data_store[1]\n    train_battery_range=data_store[2]\n  \
            \  train_y_soh=data_store[3]\n    y_norm=data_store[4]\n    test_x=data_store[5]\n\
            \    test_y=data_store[6]\n\n    model = keras.models.load_model(data_path\
            \ +'data/results/trained_model/%s.h5' % experiment_name)\n    model.summary(expand_nested=True)\n\
            \n    logging.info(\"Training mode\")\n    results = model.evaluate(test_x,\
            \ test_y, return_dict = True)\n    logging.info(results)\n    max_rmse\
            \ = 0\n    for index in range(test_x.shape[0]):\n        result = model.evaluate(np.array([test_x[index,\
            \ :, :]]), np.array([test_y[index]]), return_dict = True, verbose=0)\n\
            \        max_rmse = max(max_rmse, result['rmse'])\n\n    logging.info(\"\
            Max rmse: {}\".format(max_rmse))\n\n    train_predictions = model.predict(train_x)\n\
            \    train_y = y_norm.denormalize(train_y)\n    train_predictions = y_norm.denormalize(train_predictions)\n\
            \    a = 0\n    for b in train_battery_range:\n        fig = go.Figure()\n\
            \        fig.add_trace(go.Scatter(x=train_y_soh[a:b], y=train_predictions[a:b,0],\n\
            \                            mode='lines', name='predicted'))\n      \
            \  fig.add_trace(go.Scatter(x=train_y_soh[a:b], y=train_y[a:b],\n    \
            \                        mode='lines', name='actual'))\n        fig.update_layout(title='Results\
            \ on training',\n                        xaxis_title='SoH Capacity',\n\
            \                        yaxis_title='Remaining Ah until EOL',\n     \
            \                   xaxis={'autorange':'reversed'},\n                \
            \        width=1400,\n                    height=600)\n        # fig.show()\n\
            \        output_image = data_path+'data/results/trained_model/%s.png'\
            \ % experiment_name\n        fig.write_image(output_image,format='png')\n\
            \n        output_image = data_path+'data/results/trained_model/%s.png'\
            \ % experiment_name\n\n    endpoint_url=os.environ[\"s3_host\"]\n    aws_access_key_id=os.environ[\"\
            s3_access_key\"]\n    aws_secret_access_key=os.environ[\"s3_secret_access_key\"\
            ]\n    logging.info(\"S3 creds %s %s %s \",endpoint_url,aws_access_key_id,\
            \ aws_secret_access_key)\n    logging.info(\"Uploading model to %s file\
            \ %s \",model_bucket,experiment_name)\n\n    s3_target = boto3.resource('s3',\n\
            \        endpoint_url=endpoint_url,\n        aws_access_key_id=aws_access_key_id,\n\
            \        aws_secret_access_key=aws_secret_access_key,\n        aws_session_token=None,\n\
            \        config=boto3.session.Config(signature_version='s3v4'),\n    \
            \    verify=False\n    )\n\n    with zipfile.ZipFile('/tmp/'+experiment_name+'.zip',\
            \ mode=\"w\") as myzip:\n        myzip.write(data_path +'data/results/trained_model/%s.h5'\
            \ % experiment_name)\n\n    with open('/tmp/'+experiment_name+'.zip',\
            \ 'rb') as f:\n        s3_target.meta.client.upload_fileobj(f,model_bucket,\
            \ experiment_name+\".zip\")\n\n    client = mqtt_client.Client(client_id=\"\
            model_upload_notify\", userdata=None, transport=\"tcp\")\n    client.enable_logger(logger=logging)\n\
            \    client.tls_set(ca_certs=broker_cert)\n    client.tls_insecure_set(True)\n\
            \    client.username_pw_set('admin', 'admin_access.redhat.com')\n    client.connect(host='mqtt-broker-acc1-0-svc.battery-monitoring.svc',\
            \ port=1883)\n\n    payload={\n        \"url\": \"http://rook-ceph-rgw-ceph-object-store-openshift-storage.apps.cluster.a-proof-of-concept.com/\"\
            ,\n        \"bucket\": model_bucket,\n        \"file\":  experiment_name+\"\
            .h5\",\n        \"timestamp\": datetime.timestamp(datetime.now()),\n \
            \       \"app_id\": \"modelbuildpipeline\"\n    }\n\n    jsonmsg = json.dumps(payload)\n\
            \    ret = client.publish(topic,payload=jsonmsg,qos=1)\n    status = ret[0]\n\
            \    if status == 0:\n        logging.info(f\"Send new model notification\
            \ `{jsonmsg}` to topic `{topic}`\")\n    else:\n        logging.info(f\"\
            Failed to send new model notification to topic {topic}\")\n\nimport argparse\n\
            _parser = argparse.ArgumentParser(prog='Model upload notify', description='Upload\
            \ model and notify')\n_parser.add_argument(\"--data-path\", dest=\"data_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --paramater-data\", dest=\"paramater_data\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--experiment-name\"\
            , dest=\"experiment_name\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--model-bucket\", dest=\"model_bucket\", type=str,\
            \ required=False, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
            \n_outputs = model_upload_notify(**_parsed_args)\n"
          env:
          - name: mqtt_broker
            value: mqtt-broker-acc1-0-svc.battery-monitoring.svc
          - name: mqtt_port
            value: '1883'
          - name: mqtt_cert
            value: /opt/certs/public.cert
          - name: s3_host
            value: http://rook-ceph-rgw-ceph-object-store.openshift-storage.svc:8080
          - name: s3_access_key
            valueFrom:
              secretKeyRef:
                key: AWS_ACCESS_KEY_ID
                name: battery-model-bucket
          - name: s3_secret_access_key
            valueFrom:
              secretKeyRef:
                key: AWS_SECRET_ACCESS_KEY
                name: battery-model-bucket
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
          image: registry.access.redhat.com/ubi8/python-38
          volumeMounts:
          - mountPath: /opt/data/pitstop
            name: batterydatavol
          - mountPath: /opt/certs/
            name: mqttcert
        params:
        - name: load-trigger-data-Output
        - name: prep-data-train-model-trname
        volumes:
        - name: batterydatavol
          persistentVolumeClaim:
            claimName: batterydata
        - name: mqttcert
          secret:
            secretName: mqtt-cert-secret
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Model upload
              notify", "outputs": [], "version": "Model upload notify@sha256=83cb5f280590314586de57a1d3fd3597a9fb173e0a87158f3f898e588fd6c01c"}'
        workspaces:
        - name: model-upload-notify
      workspaces:
      - name: model-upload-notify
        workspace: edge-pipeline
      runAfter:
      - prep-data-train-model
    - name: prep-data-train-model-2
      params:
      - name: epoch_count
        value: $(params.epoch_count)
      - name: load-trigger-data-Output
        value: $(tasks.load-trigger-data.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --data-path
          - /opt/data/pitstop/
          - --epoch-count
          - $(inputs.params.epoch_count)
          - --experiment-name
          - $(inputs.params.load-trigger-data-Output)
          - --run-mode
          - '2'
          - --parameter-data
          - $(workspaces.prep-data-train-model-2.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/parameter_data
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n\
            \    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return\
            \ file_path\n\ndef prep_data_train_model(data_path, epoch_count, parameter_data,\
            \ experiment_name, run_mode=0):\n    \"\"\"Preps the data for processing\"\
            \"\"\n    import numpy as np\n    import pandas as pd\n    import sys\n\
            \    import pickle\n    import logging\n    from importlib import reload\n\
            \    from tensorflow import keras\n    from keras import layers, regularizers\n\
            \    from keras.models import Model\n    from keras import backend as\
            \ K\n    from keras.models import Sequential, Model\n    from keras.layers\
            \ import Dense, Dropout, Activation, TimeDistributed, Input, Concatenate\n\
            \    from keras.optimizers import Adam\n    from keras.layers import LSTM,\
            \ Masking\n    from data_processing.unibo_powertools_data import UniboPowertoolsData,\
            \ CycleCols\n    from data_processing.model_data_handler import ModelDataHandler\n\
            \    from data_processing.prepare_rul_data import RulHandler\n    sys.path.append(data_path)\n\
            \    reload(logging)\n    logging.basicConfig(format='%(asctime)s [%(levelname)s]:\
            \ %(message)s', level=logging.INFO, datefmt='%Y/%m/%d %H:%M:%S')\n   \
            \ #help=\"0 normal Training (default), 1 Bad Training, 2 Inference\",\
            \ default=0)\n\n    # Normal Training\n    if run_mode == 0:\n       \
            \ logging.info(\"Normal Training\")\n        train_names = [ ]\n     \
            \   test_names = [ ]\n        # epoch_count=270\n\n    elif run_mode ==\
            \ 1: # POOR TRAINING\n        logging.info(\"Poor Training\")\n      \
            \  train_names = [ ]\n\n        test_names = [ ]\n        # epoch_count=40\n\
            \    else: #INFERENCING\n        logging.info(\"Inferencing\")\n     \
            \   train_names = [ ]\n        test_names = [ ]\n\n    # # Load Data\n\
            \n    dataset = UniboPowertoolsData(\n        test_types=[],\n       \
            \ chunk_size=1000000,\n        lines=[37, 40],\n        charge_line=37,\n\
            \        discharge_line=40,\n        base_path=data_path\n    )\n\n  \
            \  ################################################NOC\n    dataset.prepare_data(train_names,\
            \ test_names)\n    dataset_handler = ModelDataHandler(dataset, [\n   \
            \     CycleCols.VOLTAGE,\n        CycleCols.CURRENT,\n        CycleCols.TEMPERATURE\n\
            \    ])\n\n    rul_handler = RulHandler()\n\n    # # Data preparation\n\
            \n    capacity_tresholds = { }\n\n    (train_x, train_y_soh, test_x, test_y_soh,\n\
            \    train_battery_range, test_battery_range,\n    time_train, time_test,\
            \ current_train, current_test) = dataset_handler.get_discharge_whole_cycle_future(train_names,\
            \ test_names, min_cycle_length=300)\n\n    train_y = rul_handler.prepare_y_future(train_names,\
            \ train_battery_range, train_y_soh, current_train, time_train, capacity_tresholds)\n\
            \    test_y = rul_handler.prepare_y_future(test_names, test_battery_range,\
            \ test_y_soh, current_test, time_test, capacity_tresholds)\n    x_norm\
            \ = rul_handler.Normalization()\n    x_norm.fit(train_x)\n\n    train_x\
            \ = x_norm.normalize(train_x)\n    test_x = x_norm.normalize(test_x)\n\
            \n    AUTOENCODER_WEIGHTS = '2023-02-09-15-50-22_autoencoder_gl_unibo_powertools'\n\
            \    N_CYCLE = 500\n    WARMUP_TRAIN = 15\n    WARMUP_TEST = 30\n\n  \
            \  opt = keras.optimizers.Adam(learning_rate=0.0002)\n    LATENT_DIM =\
            \ 10\n\n    class Autoencoder(Model):\n        def __init__(self, latent_dim):\n\
            \            super(Autoencoder, self).__init__()\n            self.latent_dim\
            \ = latent_dim\n\n            encoder_inputs = layers.Input(shape=(train_x.shape[1],\
            \ train_x.shape[2]))\n            encoder_conv1 = layers.Conv1D(filters=8,\
            \ kernel_size=10, strides=2, activation='relu', padding='same')(encoder_inputs)\n\
            \            encoder_pool1 = layers.MaxPooling1D(5, padding='same')(encoder_conv1)\n\
            \            encoder_conv2 = layers.Conv1D(filters=8, kernel_size=4, strides=1,\
            \ activation='relu', padding='same')(encoder_pool1)\n            encoder_pool2\
            \ = layers.MaxPooling1D(3, padding='same')(encoder_conv2)\n          \
            \  encoder_flat1 = layers.Flatten()(encoder_pool1)\n            encoder_flat2\
            \ = layers.Flatten()(encoder_pool2)\n            encoder_concat = layers.concatenate([encoder_flat1,\
            \ encoder_flat2])\n            encoder_outputs = layers.Dense(self.latent_dim,\
            \ activation='relu')(encoder_concat)\n            self.encoder = Model(inputs=encoder_inputs,\
            \ outputs=encoder_outputs)\n\n            decoder_inputs = layers.Input(shape=(self.latent_dim,))\n\
            \            decoder_dense1 = layers.Dense(10*8, activation='relu')(decoder_inputs)\n\
            \            decoder_reshape1 = layers.Reshape((10, 8))(decoder_dense1)\n\
            \            decoder_upsample1 = layers.UpSampling1D(3)(decoder_reshape1)\n\
            \            decoder_convT1 = layers.Conv1DTranspose(filters=8, kernel_size=4,\
            \ strides=1, activation='relu', padding='same')(decoder_upsample1)\n \
            \           decoder_upsample2 = layers.UpSampling1D(5)(decoder_convT1)\n\
            \            decoder_convT2 = layers.Conv1DTranspose(filters=8, kernel_size=10,\
            \ strides=2, activation='relu', padding='same')(decoder_upsample2)\n \
            \           decoder_outputs = layers.Conv1D(3, kernel_size=3, activation='relu',\
            \ padding='same')(decoder_convT2)\n            self.decoder = Model(inputs=decoder_inputs,\
            \ outputs=decoder_outputs)\n\n        def call(self, x):\n           \
            \ encoded = self.encoder(x)\n            decoded = self.decoder(encoded)\n\
            \            return decoded\n\n    autoencoder = Autoencoder(LATENT_DIM)\n\
            \n    autoencoder.compile(optimizer=opt, loss='mse', metrics=['mse', 'mae',\
            \ 'mape', keras.metrics.RootMeanSquaredError(name='rmse')])\n    autoencoder.encoder.summary()\n\
            \    autoencoder.decoder.summary()\n    autoencoder.load_weights(data_path\
            \ + 'data/results/trained_model/%s/model' % AUTOENCODER_WEIGHTS)\n   \
            \ # compression\n    train_x = autoencoder.encoder(train_x).numpy()\n\
            \    test_x = autoencoder.encoder(test_x).numpy()\n    logging.info(\"\
            compressed train x shape {}\".format(train_x.shape))\n    logging.info(\"\
            compressed test x shape {}\".format(test_x.shape))\n    test_x = test_x[:,~np.all(train_x\
            \ == 0, axis=0)]#we need same column number of training\n    train_x =\
            \ train_x[:,~np.all(train_x == 0, axis=0)]\n    logging.info(\"compressed\
            \ train x shape without zero column {}\".format(train_x.shape))\n    logging.info(\"\
            compressed test x shape without zero column {}\".format(test_x.shape))\n\
            \n    x_norm = rul_handler.Normalization()\n    x_norm.fit(train_x)\n\
            \    train_x = x_norm.normalize(train_x)\n    test_x = x_norm.normalize(test_x)\n\
            \    train_x = rul_handler.battery_life_to_time_series(train_x, N_CYCLE,\
            \ train_battery_range)\n    test_x = rul_handler.battery_life_to_time_series(test_x,\
            \ N_CYCLE, test_battery_range)\n    train_x, train_y, train_battery_range,\
            \ train_y_soh = rul_handler.delete_initial(train_x, train_y, train_battery_range,\
            \ train_y_soh, WARMUP_TRAIN)\n    test_x, test_y, test_battery_range,\
            \ test_y_soh = rul_handler.delete_initial(test_x, test_y, test_battery_range,\
            \ test_y_soh, WARMUP_TEST)\n\n    # first one is SOH, we keep only RUL\n\
            \    train_y = train_y[:,1]\n    test_y = test_y[:,1]\n\n    # # Y normalization\n\
            \    y_norm = rul_handler.Normalization()\n    y_norm.fit(train_y)\n \
            \   train_y = y_norm.normalize(train_y)\n    test_y = y_norm.normalize(test_y)\
            \  \n\n    ## Only when training\n    if run_mode != 2:\n        opt =\
            \ keras.optimizers.Adam(lr=0.000003)\n        model = Sequential()\n \
            \       model.add(Masking(input_shape=(train_x.shape[1], train_x.shape[2])))\n\
            \        model.add(LSTM(128, activation='tanh',\n                    return_sequences=True,\n\
            \                    kernel_regularizer=regularizers.l2(0.0002)))\n  \
            \      model.add(LSTM(64, activation='tanh', return_sequences=False,\n\
            \                    kernel_regularizer=regularizers.l2(0.0002)))\n  \
            \      model.add(Dense(64, activation='selu', kernel_regularizer=regularizers.l2(0.0002)))\n\
            \        model.add(Dense(32, activation='selu', kernel_regularizer=regularizers.l2(0.0002)))\n\
            \        model.add(Dense(1, activation='linear'))\n        model.summary()\n\
            \n        model.compile(optimizer=opt, loss='huber', metrics=['mse', 'mae',\
            \ 'mape', keras.metrics.RootMeanSquaredError(name='rmse')])\n\n      \
            \  history = model.fit(train_x, train_y, \n                          \
            \      epochs=epoch_count, \n                                batch_size=32,\n\
            \                                verbose=2,\n                        \
            \        validation_split=0.1\n                            )\n       \
            \ model_path= data_path+'data/results/trained_model/%s.h5' % experiment_name\n\
            \n        model.save(model_path)\n        logging.info(\"Model saved to\
            \ %s\",model_path)\n\n        hist_df = pd.DataFrame(history.history)\n\
            \        hist_csv_file = data_path+'data/results/trained_model/%s_history.csv'\
            \ % experiment_name\n        with open(hist_csv_file, mode='w') as f:\n\
            \            hist_df.to_csv(f)\n\n    data_store = [train_x, train_y,\
            \ train_battery_range, train_y_soh, y_norm, test_x, test_y]\n\n    with\
            \ open(parameter_data, \"b+w\") as f:\n        pickle.dump(data_store,f)\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Prep data train\
            \ model', description='Preps the data for processing')\n_parser.add_argument(\"\
            --data-path\", dest=\"data_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--epoch-count\", dest=\"epoch_count\", type=int,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--experiment-name\"\
            , dest=\"experiment_name\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--run-mode\", dest=\"run_mode\", type=int, required=False,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--parameter-data\"\
            , dest=\"parameter_data\", type=_make_parent_dirs_and_return_path, required=True,\
            \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
            \n_outputs = prep_data_train_model(**_parsed_args)\n"
          image: registry.access.redhat.com/ubi8/python-38
          volumeMounts:
          - mountPath: /opt/data/pitstop
            name: batterydatavol
          env:
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
        - image: busybox
          name: output-taskrun-name
          command:
          - sh
          - -ec
          - echo -n "$(context.taskRun.name)" > "$(results.taskrun-name.path)"
        - image: busybox
          name: copy-results-artifacts
          command:
          - sh
          - -ec
          - |
            set -exo pipefail
            TOTAL_SIZE=0
            copy_artifact() {
            if [ -d "$1" ]; then
              tar -czvf "$1".tar.gz "$1"
              SUFFIX=".tar.gz"
            fi
            ARTIFACT_SIZE=`wc -c "$1"${SUFFIX} | awk '{print $1}'`
            TOTAL_SIZE=$( expr $TOTAL_SIZE + $ARTIFACT_SIZE)
            touch "$2"
            if [[ $TOTAL_SIZE -lt 3072 ]]; then
              if [ -d "$1" ]; then
                tar -tzf "$1".tar.gz > "$2"
              elif ! awk "/[^[:print:]]/{f=1} END{exit !f}" "$1"; then
                cp "$1" "$2"
              fi
            fi
            }
            copy_artifact $(workspaces.prep-data-train-model-2.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/parameter_data $(results.parameter-data.path)
          onError: continue
          env:
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
        params:
        - name: epoch_count
        - name: load-trigger-data-Output
        results:
        - name: parameter-data
          type: string
          description: /tmp/outputs/parameter_data/data
        - name: taskrun-name
          type: string
        volumes:
        - name: batterydatavol
          persistentVolumeClaim:
            claimName: batterydata
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Prep data train
              model", "outputs": [{"name": "parameter_data"}], "version": "Prep data
              train model@sha256=fc6806ccee5930fde6985ee26a7dcc4a9e55262b39679fcd827a438e955fba54"}'
        workspaces:
        - name: prep-data-train-model-2
      runAfter:
      - model-upload-notify
      workspaces:
      - name: prep-data-train-model-2
        workspace: edge-pipeline
    - name: model-inference
      params:
      - name: VIN
        value: $(params.VIN)
      - name: load-trigger-data-Output
        value: $(tasks.load-trigger-data.results.Output)
      - name: prep-data-train-model-2-trname
        value: $(tasks.prep-data-train-model-2.results.taskrun-name)
      taskSpec:
        steps:
        - name: main
          args:
          - --data-path
          - /opt/data/pitstop/
          - --paramater-data
          - $(workspaces.model-inference.path)/artifacts/$ORIG_PR_NAME/$(params.prep-data-train-model-2-trname)/parameter_data
          - --experiment-name
          - $(inputs.params.load-trigger-data-Output)
          - --vin
          - $(inputs.params.VIN)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def model_inference(data_path,paramater_data,experiment_name,vin=\"12345\"\
            ):\n    \"\"\"Evaluate the model\"\"\"\n    import os  \n    import logging\n\
            \    from importlib import reload\n    import pickle\n    import time\n\
            \    import numpy as np\n    import pandas as pd\n    import plotly.graph_objects\
            \ as go\n    import json\n    import random\n    import tensorflow as\
            \ tf\n    from paho.mqtt import client as mqtt_client\n    from tensorflow\
            \ import keras\n    from keras import layers, regularizers\n    from keras\
            \ import backend as K\n    from keras.models import Sequential, Model\n\
            \    from keras.layers import Dense, Dropout, Activation, TimeDistributed,\
            \ Input, Concatenate\n    from keras.optimizers import Adam\n    from\
            \ keras.layers import LSTM, Masking\n\n    reload(logging)\n    logging.basicConfig(format='%(asctime)s\
            \ [%(levelname)s]: %(message)s', level=logging.INFO, datefmt='%Y/%m/%d\
            \ %H:%M:%S')\n\n    broker=os.getenv(\"mqtt_broker\",\"not set\")\n  \
            \  port=os.getenv(\"mqtt_port\",\"-1\")\n    broker_cert=os.getenv(\"\
            mqtt_cert\",\"/opt/certs/public.cert\")\n    topic=\"batterytest/batterymonitoring\"\
            \n    logging.info(\"MQTT params Broker=%s Port=%s Topic=%s\",broker,port,topic)\n\
            \n    f = open(paramater_data,\"b+r\")\n    data_store = pickle.load(f)\n\
            \    train_x=data_store[0]\n    train_y=data_store[1]\n    y_norm=data_store[4]\n\
            \n    model = keras.models.load_model(data_path +'data/results/trained_model/%s.h5'\
            \ % experiment_name)\n    model.summary(expand_nested=True)\n\n    logging.info(\"\
            Inferencing mode\")\n\n    train_predictions = model.predict(train_x)\n\
            \    train_y = y_norm.denormalize(train_y)\n    train_predictions = y_norm.denormalize(train_predictions)\n\
            \    y=train_predictions[0,0]\n\n    payload={\n        \"VIN\": vin,\n\
            \        \"Battery Lifetime AH\": str(y), \n        \"Timestamp\": str(time.time())\n\
            \    }\n\n    jsonmsg = json.dumps(payload)    \n    client = mqtt_client.Client(client_id=\"\
            model_inference\", userdata=None, transport=\"tcp\")\n    client.enable_logger(logger=logging)\n\
            \    client.tls_set(ca_certs=broker_cert)\n    client.tls_insecure_set(True)\n\
            \    client.username_pw_set('ricka', 'never_gonna_give_you_up')\n    client.connect(host='mqtt-broker-acc1-0-svc.battery-monitoring.svc',\
            \ port=1883)    \n\n    result=client.publish(topic,jsonmsg,qos=1)\n \
            \   status = result[0]\n    if status == 0:\n        logging.info(f\"\
            Send `{jsonmsg}` to topic `{topic}`\")\n    else:\n        logging.info(f\"\
            Failed to send message to topic {topic}\")  \n\nimport argparse\n_parser\
            \ = argparse.ArgumentParser(prog='Model inference', description='Evaluate\
            \ the model')\n_parser.add_argument(\"--data-path\", dest=\"data_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --paramater-data\", dest=\"paramater_data\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--experiment-name\"\
            , dest=\"experiment_name\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--vin\", dest=\"vin\", type=str, required=False,\
            \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
            \n_outputs = model_inference(**_parsed_args)\n"
          env:
          - name: mqtt_broker
            value: mqtt-broker-acc1-0-svc.battery-monitoring.svc
          - name: mqtt_port
            value: '1883'
          - name: mqtt_cert
            value: /opt/certs/public.cert
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
          image: registry.access.redhat.com/ubi8/python-38
          volumeMounts:
          - mountPath: /opt/data/pitstop
            name: batterydatavol
          - mountPath: /opt/certs/
            name: mqttcert
        params:
        - name: VIN
        - name: load-trigger-data-Output
        - name: prep-data-train-model-2-trname
        volumes:
        - name: batterydatavol
          persistentVolumeClaim:
            claimName: batterydata
        - name: mqttcert
          secret:
            secretName: mqtt-cert-secret
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Model inference",
              "outputs": [], "version": "Model inference@sha256=cbb5162fb54bab020556369149ac0a8b36bc003924bb79b11f40bc03263e0d11"}'
        workspaces:
        - name: model-inference
      workspaces:
      - name: model-inference
        workspace: edge-pipeline
      runAfter:
      - prep-data-train-model-2
    workspaces:
    - name: edge-pipeline
  taskRunSpecs:
  - pipelineTaskName: prep-data-train-model
    taskPodTemplate:
      tolerations:
      - effect: NoSchedule
        key: nvidia.com/gpu
        operator: Equal
        value: "true"
      nodeSelector:
        nvidia.com/gpu.present: "true"
  workspaces:
  - name: edge-pipeline
    volumeClaimTemplate:
      spec:
        storageClassName: managed-csi
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
