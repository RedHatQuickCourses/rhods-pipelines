apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: offline-scoring-kfp
  annotations:
    tekton.dev/output_artifacts: '{"load-model": [{"key": "artifacts/$PIPELINERUN/load-model/model_path.tgz",
      "name": "load-model-model_path", "path": "/tmp/outputs/model_path/data"}]}'
    tekton.dev/input_artifacts: '{"predict": [{"name": "load-model-model_path", "parent_task":
      "load-model"}]}'
    tekton.dev/artifact_bucket: mlpipeline
    tekton.dev/artifact_endpoint: minio-service.kubeflow:9000
    tekton.dev/artifact_endpoint_scheme: http://
    tekton.dev/artifact_items: '{"data-ingestion": [], "load-model": [["model_path",
      "$(results.model-path.path)"]], "predict": [], "preprocessing": [], "upload-results":
      []}'
    sidecar.istio.io/inject: "false"
    tekton.dev/track_artifact: "true"
    tekton.dev/template: ''
    pipelines.kubeflow.org/big_data_passing_format: $(workspaces.$TASK_NAME.path)/artifacts/$ORIG_PR_NAME/$TASKRUN_NAME/$TASK_PARAM_NAME
    pipelines.kubeflow.org/pipeline_spec: '{"inputs": [{"default": "live-data.csv",
      "name": "data_object_name", "optional": true, "type": "String"}, {"default":
      "model-latest.onnx", "name": "model_object_name", "optional": true, "type":
      "String"}], "name": "offline-scoring-kfp"}'
  labels:
    pipelines.kubeflow.org/pipelinename: ''
    pipelines.kubeflow.org/generation: ''
spec:
  params:
  - name: data_object_name
    value: live-data.csv
  - name: model_object_name
    value: model-latest.onnx
  pipelineSpec:
    params:
    - name: data_object_name
      default: live-data.csv
    - name: model_object_name
      default: model-latest.onnx
    tasks:
    - name: data-ingestion
      params:
      - name: data_object_name
        value: $(params.data_object_name)
      taskSpec:
        steps:
        - name: main
          args:
          - --data-object-name
          - $(inputs.params.data_object_name)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def data_ingestion(data_object_name):
                from os import environ

                from boto3 import client

                raw_data_file_location = '/data/raw_data.csv'

                print('Commencing data ingestion.')

                s3_endpoint_url = environ.get('AWS_S3_ENDPOINT')
                s3_access_key = environ.get('AWS_ACCESS_KEY_ID')
                s3_secret_key = environ.get('AWS_SECRET_ACCESS_KEY')
                s3_bucket_name = environ.get('AWS_S3_BUCKET')

                print(f'Downloading data "{data_object_name}" '
                    f'from bucket "{s3_bucket_name}" '
                    f'from S3 storage at {s3_endpoint_url}'
                    f'to {raw_data_file_location}')

                s3_client = client(
                    's3', endpoint_url= s3_endpoint_url,
                    aws_access_key_id=s3_access_key, aws_secret_access_key=s3_secret_key
                )

                s3_client.download_file(
                    s3_bucket_name,
                    data_object_name,
                    raw_data_file_location
                )
                print('Finished data ingestion.')

            import argparse
            _parser = argparse.ArgumentParser(prog='Data ingestion', description='')
            _parser.add_argument("--data-object-name", dest="data_object_name", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = data_ingestion(**_parsed_args)
          env:
          - name: AWS_S3_ENDPOINT
            valueFrom:
              secretKeyRef:
                key: AWS_S3_ENDPOINT
                name: aws-connection-rhods-enablement-s3
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: AWS_ACCESS_KEY_ID
                name: aws-connection-rhods-enablement-s3
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: AWS_SECRET_ACCESS_KEY
                name: aws-connection-rhods-enablement-s3
          - name: AWS_S3_BUCKET
            valueFrom:
              secretKeyRef:
                key: AWS_S3_BUCKET
                name: aws-connection-rhods-enablement-s3
          image: quay.io/mmurakam/runtimes:fraud-detection-v0.2.0
          volumeMounts:
          - mountPath: /data
            name: offline-scoring-data-volume
        params:
        - name: data_object_name
        volumes:
        - name: offline-scoring-data-volume
          persistentVolumeClaim:
            claimName: model-training-data-volume
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Data ingestion",
              "outputs": [], "version": "Data ingestion@sha256=209ab56624a7a04d43c2dab68a54f57584130bad22abc363e3025d81f05c4d55"}'
    - name: preprocessing
      taskSpec:
        steps:
        - name: main
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def preprocessing():
                from numpy import save
                from pandas import read_csv
                from sklearn.preprocessing import RobustScaler

                print('Preprocessing data.')

                raw_data_file_location = '/data/raw_data.csv'
                features_file_location = '/data/features.npy'
                df = read_csv(raw_data_file_location)

                rob_scaler = RobustScaler()

                df['scaled_amount'] = rob_scaler.fit_transform(
                    df['Amount'].values.reshape(-1, 1)
                )
                df['scaled_time'] = rob_scaler.fit_transform(
                    df['Time'].values.reshape(-1, 1)
                )
                df.drop(['Time', 'Amount'], axis=1, inplace=True)
                scaled_amount = df['scaled_amount']
                scaled_time = df['scaled_time']

                df.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)
                df.insert(0, 'scaled_amount', scaled_amount)
                df.insert(1, 'scaled_time', scaled_time)

                save(features_file_location, df.values)

                print('data processing done')

            import argparse
            _parser = argparse.ArgumentParser(prog='Preprocessing', description='')
            _parsed_args = vars(_parser.parse_args())

            _outputs = preprocessing(**_parsed_args)
          image: quay.io/mmurakam/runtimes:fraud-detection-v0.2.0
          volumeMounts:
          - mountPath: /data
            name: offline-scoring-data-volume
        volumes:
        - name: offline-scoring-data-volume
          persistentVolumeClaim:
            claimName: model-training-data-volume
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Preprocessing",
              "outputs": [], "version": "Preprocessing@sha256=a79b0e3c7936604822198b3be2ce1d6438425d31dcb6c28fd6256ef298367434"}'
      runAfter:
      - data-ingestion
    - name: load-model
      params:
      - name: model_object_name
        value: $(params.model_object_name)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-object-name
          - $(inputs.params.model_object_name)
          - '----output-paths'
          - $(results.model-path.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def load_model(model_object_name):
                from os import environ

                from boto3 import client

                print('Commencing model loading.')
                model_path = '/data/model.onnx'

                s3_endpoint_url = environ.get('AWS_S3_ENDPOINT')
                s3_access_key = environ.get('AWS_ACCESS_KEY_ID')
                s3_secret_key = environ.get('AWS_SECRET_ACCESS_KEY')
                s3_bucket_name = environ.get('AWS_S3_BUCKET')

                print(f'Downloading model "{model_object_name}" '
                      f'from bucket {s3_bucket_name} '
                      f'from S3 storage at {s3_endpoint_url}')

                s3_client = client(
                    's3', endpoint_url= s3_endpoint_url,
                    aws_access_key_id=s3_access_key, aws_secret_access_key=s3_secret_key
                )

                s3_client.download_file(
                    s3_bucket_name, model_object_name, model_path
                )

                print('Finished model loading.')
                return [model_path]

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import argparse
            _parser = argparse.ArgumentParser(prog='Load model', description='')
            _parser.add_argument("--model-object-name", dest="model_object_name", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = load_model(**_parsed_args)

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          env:
          - name: AWS_S3_ENDPOINT
            valueFrom:
              secretKeyRef:
                key: AWS_S3_ENDPOINT
                name: aws-connection-rhods-enablement-s3
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: AWS_ACCESS_KEY_ID
                name: aws-connection-rhods-enablement-s3
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: AWS_SECRET_ACCESS_KEY
                name: aws-connection-rhods-enablement-s3
          - name: AWS_S3_BUCKET
            valueFrom:
              secretKeyRef:
                key: AWS_S3_BUCKET
                name: aws-connection-rhods-enablement-s3
          image: quay.io/mmurakam/runtimes:fraud-detection-v0.2.0
          volumeMounts:
          - mountPath: /data
            name: offline-scoring-data-volume
        params:
        - name: model_object_name
        results:
        - name: model-path
          type: string
          description: /tmp/outputs/model_path/data
        volumes:
        - name: offline-scoring-data-volume
          persistentVolumeClaim:
            claimName: model-training-data-volume
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Load model",
              "outputs": [{"name": "model_path", "type": "String"}], "version": "Load
              model@sha256=0f0edf1d5d66eb938db1f604c98c053e103784f2d64775663e5ba481dfdb4411"}'
    - name: predict
      params:
      - name: load-model-model_path
        value: $(tasks.load-model.results.model-path)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.load-model-model_path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def predict(model_path):
                from numpy import argmax, array, load
                from onnxruntime import InferenceSession
                from pandas import DataFrame

                print('Commencing offline scoring.')

                X = load('/data/features.npy').astype('float32')

                session = InferenceSession(model_path)
                raw_results = session.run([], {'dense_input': X})[0]

                results = argmax(raw_results, axis=1)
                class_map_array = array(['no fraud', 'fraud'])
                mapped_results = class_map_array[results]

                print(f'Scored data set. Writing report.')

                column_names = [f'V{i}' for i in range(1, 31)]
                report = DataFrame(X, columns=column_names)
                report.insert(0, 'Prediction', mapped_results)

                report.to_csv(f'/data/predictions.csv')

                print('Wrote report. Offline scoring complete.')

            import argparse
            _parser = argparse.ArgumentParser(prog='Predict', description='')
            _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = predict(**_parsed_args)
          image: quay.io/mmurakam/runtimes:fraud-detection-v0.2.0
          volumeMounts:
          - mountPath: /data
            name: offline-scoring-data-volume
        params:
        - name: load-model-model_path
        volumes:
        - name: offline-scoring-data-volume
          persistentVolumeClaim:
            claimName: model-training-data-volume
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Predict", "outputs":
              [], "version": "Predict@sha256=8947389ed0ad663af88f877090401847772c14349afe8b220207aaee80aae3f9"}'
      runAfter:
      - preprocessing
      - load-model
    - name: upload-results
      taskSpec:
        steps:
        - name: main
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def upload_results():
                from datetime import datetime
                from os import environ

                from boto3 import client

                print('Commencing results upload.')

                s3_endpoint_url = environ.get('AWS_S3_ENDPOINT')
                s3_access_key = environ.get('AWS_ACCESS_KEY_ID')
                s3_secret_key = environ.get('AWS_SECRET_ACCESS_KEY')
                s3_bucket_name = environ.get('AWS_S3_BUCKET')

                timestamp = datetime.now().strftime('%y%m%d%H%M')
                results_name = f'predictions-{timestamp}.csv'

                print(f'Uploading predictions to bucket {s3_bucket_name} '
                      f'to S3 storage at {s3_endpoint_url}')

                s3_client = client(
                    's3', endpoint_url= s3_endpoint_url,
                    aws_access_key_id=s3_access_key, aws_secret_access_key=s3_secret_key
                )

                with open(f'/data/predictions.csv', 'rb') as results_file:
                    s3_client.upload_fileobj(results_file, s3_bucket_name, results_name)

                print('Finished uploading results.')

            import argparse
            _parser = argparse.ArgumentParser(prog='Upload results', description='')
            _parsed_args = vars(_parser.parse_args())

            _outputs = upload_results(**_parsed_args)
          env:
          - name: AWS_S3_ENDPOINT
            valueFrom:
              secretKeyRef:
                key: AWS_S3_ENDPOINT
                name: aws-connection-rhods-enablement-s3
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: AWS_ACCESS_KEY_ID
                name: aws-connection-rhods-enablement-s3
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: AWS_SECRET_ACCESS_KEY
                name: aws-connection-rhods-enablement-s3
          - name: AWS_S3_BUCKET
            valueFrom:
              secretKeyRef:
                key: AWS_S3_BUCKET
                name: aws-connection-rhods-enablement-s3
          image: quay.io/mmurakam/runtimes:fraud-detection-v0.2.0
          volumeMounts:
          - mountPath: /data
            name: offline-scoring-data-volume
        volumes:
        - name: offline-scoring-data-volume
          persistentVolumeClaim:
            claimName: model-training-data-volume
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Upload results",
              "outputs": [], "version": "Upload results@sha256=fd1610f7750985d4b7eaef78f7d3e12c256808e529e554db0d821f1d0e8023c3"}'
      runAfter:
      - predict
