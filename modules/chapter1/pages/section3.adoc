= Data Science Pipelines Using Kubeflow in Red Hat OpenShift AI

Red Hat OpenShift AI offers two out-of-the-box mechanisms to work with Data Science Pipelines in terms of building and triggering pipelines.

The first mechanism is the _Elyra Pipelines_ JupyterLab extension, which provides a visual editor for creating pipelines based on Jupyter notebooks as well as Python or R scripts. 

The second mechanism and the one discussed here is based on the Kubeflow Pipelines SDK. With the SDK, pipelines are built using Python scripts and submitted to the Data Science Pipelines runtime to be scheduled for execution.

[NOTE]
====
There is work in progress to integrate Kubeflow Pipelines version 2 in Data Science Pipelines. At the time of writing Kubeflow Pipelines version 1 is used, which needs to be considered when referring to the SDK documentation.
====

In Red Hat OpenShift AI, we use the *_Tekton_* runtime to execute pipelines, which is why the Python script needs to be compiled into a Tekton definition before it can be submitted to the runtime.

In Red Hat OpenShift AI, the Data Science Pipeline runtime consists of the following components:

* A Data Science Pipeline Server container. 
* A MariaDB for storing pipeline definitions and results.
* A Pipeline scheduler for scheduling pipeline runs.
* A Persistent Agent to record the set of containers that executed as well as their inputs and outputs.

Steps in the pipeline are executed as ephemeral pods (one per step).

[NOTE]
====
DS Pipelines in Red Hat OpenShift AI are managed by the `data-science-pipelines-operator-controller-manager` operator in the `redhat-ods-applications` namespace. The CR is an instance of _datasciencepipelinesapplications.datasciencepipelinesapplications.opendatahub.io_. Pipeline _runs_ are instances of _tekton.dev/v1 PipelineRun_.
====

== Creating a Data Science Pipeline with the KFP SDK

=== Prerequisites 
* Python Setup
[source,cmd]
----
pip install kfp==1.8.22
pip install kfp-tekton==1.8.0
----
IMPORTANT: Using the correct Python module versions is critical to avoid conflicts between the KFP SDK and Data Science Pipelines versions.

* S3 Storage

You can use any S3 compatible storage service such as _Red Hat OpenShift Data Foundation_ or _AWS S3_.
For the purpose of this enablement we will be using _Minio_.

Follow https://ai-on-openshift.io/tools-and-applications/minio/minio/[these instructions] to setup a local Minio instance on your OpenShift cluster.



=== Creating a Pipeline Server

To execute pipelines a _Pipeline server_ needs to be created, which in turn depends on the creation of an S3 Storage bucket. This is used to store the run artifacts and outputs of any pipeline run on the associated server. 
****
image::creating-data-connection.png[]
****
Once the data connection is created then the pipeline server can be created
****
image::pre-creating-pipeline-server.png[]
****
****
image::pipeline-server-created.png[]
****
=== Building and deploying a Pipeline

The following is a simplistic example of a KFP pipeline. The original is at 
https://github.com/kubeflow/kfp-tekton/blob/master/samples/flip-coin/condition.py and the example has downloaded it to a file named _coin-toss.py_

****
.Python Pipeline Example
[source,python]
----
# Copyright 2020 kubeflow.org
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from kfp import dsl
from kfp import components

def random_num(low:int, high:int) -> int:
    """Generate a random number between low and high."""
    import random
    result = random.randint(low, high)
    print(result)
    return result

def flip_coin() -> str:
    """Flip a coin and output heads or tails randomly."""
    import random
    result = 'heads' if random.randint(0, 1) == 0 else 'tails'
    print(result)
    return result

def print_msg(msg: str):
    """Print a message."""
    print(msg)


flip_coin_op = components.create_component_from_func(
    flip_coin, base_image='python:alpine3.6')
print_op = components.create_component_from_func(
    print_msg, base_image='python:alpine3.6')
random_num_op = components.create_component_from_func(
    random_num, base_image='python:alpine3.6')

@dsl.pipeline(
    name='conditional-execution-pipeline',
    description='Shows how to use dsl.Condition().'
)
def flipcoin_pipeline():
    flip = flip_coin_op()
    with dsl.Condition(flip.output == 'heads'):
        random_num_head = random_num_op(0, 9)
        with dsl.Condition(random_num_head.output > 5):
            print_op('heads and %s > 5!' % random_num_head.output)
        with dsl.Condition(random_num_head.output <= 5):
            print_op('heads and %s <= 5!' % random_num_head.output)

    with dsl.Condition(flip.output == 'tails'):
        random_num_tail = random_num_op(10, 19)
        with dsl.Condition(random_num_tail.output > 15):
            print_op('tails and %s > 15!' % random_num_tail.output)
        with dsl.Condition(random_num_tail.output <= 15):
            print_op('tails and %s <= 15!' % random_num_tail.output)


if __name__ == '__main__':
    from kfp_tekton.compiler import TektonCompiler
    TektonCompiler().compile(flipcoin_pipeline, __file__.replace('.py', '.yaml'))
----
****

To compile it into a Tekton resource definition just run the following in a terminal
[source,python]
----
python3 coin-toss.py
----

It will generate a Tekton *_Pipeline Run_* , similar to this snippet
****
[source,yaml]
----
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: conditional-execution-pipeline
  annotations:
    tekton.dev/output_artifacts: '{"flip-coin": [{"key": "artifacts/$PIPELINERUN/flip-coin/Output.tgz",
      "name": "flip-coin-Output", "path": "/tmp/outputs/Output/data"}], "random-num":
      [{"key": "artifacts/$PIPELINERUN/random-num/Output.tgz", "name": "random-num-Output",
      "path": "/tmp/outputs/Output/data"}], "random-num-2": [{"key": "artifacts/$PIPELINERUN/random-num-2/Output.tgz",
      "name": "random-num-2-Output", "path": "/tmp/outputs/Output/data"}]}'
    tekton.dev/input_artifacts: '{"print-msg": [{"name": "random-num-Output", "parent_task":
      "random-num"}], "print-msg-2": [{"name": "random-num-Output", "parent_task":
      "random-num"}], "print-msg-3": [{"name": "random-num-2-Output", "parent_task":
      "random-num-2"}], "print-msg-4": [{"name": "random-num-2-Output", "parent_task":
      "random-num-2"}]}'
    tekton.dev/artifact_bucket: mlpipeline
    tekton.dev/artifact_endpoint: minio-service.kubeflow:9000
    tekton.dev/artifact_endpoint_scheme: http://
    tekton.dev/artifact_items: '{"flip-coin": [["Output", "$(results.Output.path)"]],
      "print-msg": [], "print-msg-2": [], "print-msg-3": [], "print-msg-4": [], "random-num":
      [["Output", "$(results.Output.path)"]], "random-num-2": [["Output", "$(results.Output.path)"]]}'
    sidecar.istio.io/inject: "false"
    tekton.dev/template: ''
----
****

The resulting yaml file _(coin-toss.yaml)_ can then be uploaded throught the UI
****
image::import-pipeline.png[]
****
****
image::pipeline-imported.png[]
****
Once imported the structure of the _DAG_ will be shown. Each step in the pipeline will be run as a container on OpenShift.

****
image::pipeline-run-view.png[]
****

To execute the pipeline, click on _Create Run_ in the menu and fill out the _Name_ and _Description_.
If the pipeline has _Input Parameters_ or you need to schedule a recurring run, then that can be configured further down. Once ready, click _Create_ to submit the pipeline for scheduling.

****
image::creating-pipeline-run.png[]
****

The pipeline will execute and the outputs will be stored into the configured S3 bucket.
As the pipeline executes the view will be updated to show the steps being executed. It's possible to click on the graph nodes to reveal information of the steps.

****
image::post-pipeline-run.png[]
****

Once the pipeline has completed, it is possible to access the output and pipeline artifacts (if used) in the object storage browser of the Minio Storage UI.

****
image::object-store-after-run.png[]
****

=== Experiments And Runs

An experiment is a workspace where you can try different configurations of your pipelines. You can use experiments to organize your runs into logical groups. Experiments can contain arbitrary runs, including recurring runs. 

A run can be configured using the DSP UI or programatically using the _KFP SDK_.

image::dsp-runs.png[]

NOTE: Experiments are part of the KFP SDK and are not currently covered in this course.

== Real World Example

In this section we're going to demonstrate a real world Data Science Pipelines scenario. 

****
In this scenario we have a remote edge device which uses an AI model to manage the characteristics of its battery usage depending on the environment it's deployed in. On a regular schedule it uploads battery events via a data gateway, and those battery events are used to train a model which is then retrieved by the device and used. 


image::openshift-ai-dsp-edge.png[]
****

The entire pipeline is xref:attachment$sample-pipeline-full.py[here].

We're not going to go through all of it but focus on the key aspects of it. 

The actual pipeline is defined by the following function:

****
[source,python]
include::example$sample-pipeline-full.py[lines=445..454]
****

The '@dsl.pipeline' parameters provide the name and description if you were uploading the pipeline via an API call. The DSP UI overwrites these values.

The function *_edgetest_pipeline_* is the implementation of the pipeline.

=== Pipeline Parameters
The pipeline has four parameters:

* _file_obj_ and _src_bucket_ refer to S3 bucket details and can be ignored.
* _VIN_ is the edge device identifier and has a default value of 412356.
* _epoch_count_ is the number of training epochs to be used.

In the _Create Run_ UI these parameters are available so that users can override the values as they need.

****
image::pipeline-parameters.png[]
****

=== Pipeline Steps
The file contains the following Python functions, which roughly correspond to the steps in the diagram above

* load_trigger_data()
* prep_data_train_model()
* model_upload_notify()
* model_inference()

These functions are mapped into individual containers by using the _create_component_from_func_ function. You can specify the container _base_image_ to use as well as any additional Python packages to be installed into the container at execution time.

[source,python]
include::example$sample-pipeline-full.py[lines=433..443]

The Python functions can be used in multiple different step definitions; in the example the _prep_data_train_model_ function is used in the _prep_data_train_op_ and the _prep_inference_data_op_ containers.

The pipeline execution _graph_ is created using the following code:

[source,python]
include::example$sample-pipeline-full.py[lines=471]

[source,python]
include::example$sample-pipeline-full.py[lines=477]

[source,python]
include::example$sample-pipeline-full.py[lines=482]

[source,python]
include::example$sample-pipeline-full.py[lines=492]

[source,python]
include::example$sample-pipeline-full.py[lines=495]


[IMPORTANT] 
====
The execution order of the graph is top down but also can be controlled by using the *_.after()_* operator. 

There are other operators which control the flow of pipeline execution such as _Condition_ , _ExitHandler_, _ParallelFor_ .
These are not covered as part of this course but the https://github.com/kubeflow/pipelines/blob/master/samples/tutorials/DSL%20-%20Control%20structures/DSL%20-%20Control%20structures.py[KFP documentation] has examples.
====

The following diagram shows the order of execution.

****
image::pipeline-graph.png[]
****

This is also visible in the Red Hat OpenShift AI user interface:

****
image::oai-pipeline-graph.png[]
****

=== Pipeline Parameter Passing
As each step of our pipeline is executed in an independent container, input parameters and output values are handled as follows.

==== Input Parameters

* Simple parameters - booleans, numbers, strings - are passed by value into the container as command line arguments.
* Complex types or large amounts of data are passed via files. The value of the input parameter is the file path.

==== Output Parameters

* Output values are returned via files.

==== Passing Parameters via Files
To pass an input parameter as a file, the function argument needs to be annotated using the _InputPath_ annotation.
For returning data from a step as a file, the function argument needs to be annotated using the _OutputPath_ annotation.

In both cases the actual value of the parameter is the file path and not the actual data. So the pipeline will have to read/write to the file as necessary.

For example, in our sample pipeline we use the _parameter_data_ argument of the _prep_data_train_model_ function to return multiple data values as a file. Here's the function definition with the _OutputPath_ annotation

[source,python]
include::example$sample-pipeline-full.py[lines=51]

Here's the actual writing of the data to the file

[source,python]
include::example$sample-pipeline-full.py[lines=238..242]

This data is then consumed in the _model_upload_notify_ function, passed via the _paramater_data_ with the _InputPath_ annotation.

[source,python]
include::example$sample-pipeline-full.py[lines=243]

Reading the data

[source,python]
include::example$sample-pipeline-full.py[lines=275..276]


Linking the two functions together 

[source,python]
include::example$sample-pipeline-full.py[lines=482]


[TIP]
====
There are other parameter annotations available to handle specialised file types 
such as _InputBinaryFile_, _OutputBinaryFile_. 

The full annotation list is in the https://kubeflow-pipelines.readthedocs.io/en/1.8.22/source/kfp.components.html[KFP component documentation].

====

==== Returning multiple values from a step 
If you return a single small value from your component using the _return_ statement, the output parameter is named *_output_*.
It is, however, possible to return multiple small values using the Python _collection_ library method _namedtuple_.

From a https://github.com/kubeflow/pipelines/blob/master/samples/tutorials/Data%20passing%20in%20python%20components.ipynb[Kubeflow pipelines example]
 
[source,python]
----
def produce_two_small_outputs() -> NamedTuple('Outputs', [('text', str), ('number', int)]):
    return ("data 1", 42)
consume_task3 = consume_two_arguments(produce2_task.outputs['text'], produce2_task.outputs['number'])
----

====
The KFP SDK uses the following rules to define the input and output parameter names in your component’s interface:

    . If the argument name ends with _path and the argument is annotated as an _kfp.components.InputPath_ or _kfp.components.OutputPath_, the parameter name is the argument name with the trailing _path removed.
    . If the argument name ends with _file, the parameter name is the argument name with the trailing _file removed.
    . If you return a single small value from your component using the return statement, the output parameter is named *output*.
    . If you return several small values from your component by returning a _collections.namedtuple_, the SDK uses the tuple’s field names as the output parameter names.

    . Otherwise, the SDK uses the argument name as the parameter name.
====

[TIP]
====
In the Tekton definition you can see the definition of the _input and output artifacts_. This can be useful for debugging purposes.

include::example$sample-pipeline-full.yaml[lines=12..18]

include::example$sample-pipeline-full.yaml[lines=6..11]

include::example$sample-pipeline-full.yaml[lines=22..25]

You can also see the locations of data stored into the S3 bucket e.g. _artifacts/$PIPELINERUN/prep-data-train-model-2/parameter_data.tgz_
====

=== Execution on OpenShift

To enable the _pipeline_ to run on OpenShift we need to pass it the associated _kubernetes_ resources 

* _volumes_ 
* _environment variables_
* _node selectors, taints and tolerations_

==== Volumes
Our pipeline requires a number of volumes to be created and mounted into the executing pods. The volumes are primarily used for storage and secrets handling but can also be used for passing configuration files into the pods.

Before mounting the volumes into the pods they need to be created. The following code creates two volumes, one from a pre-existing PVC and another from a pre-existing secret.

include::example$sample-pipeline-full.py[lines=453..462]

The volumes are mounted into the containers using the *_add_pvolumes_* method:

include::example$sample-pipeline-full.py[lines=495..497]

==== Environment Variables

Environment variables can be added to the pod using the *_add_env_variable_* method. 

include::example$sample-pipeline-full.py[lines=471..475]

[NOTE]
====
The *_env_from_secret_* utility method also enables extracting values from secrets and mounting them as environment variables. In the example above the _AWS_ACCESS_KEY_ID_ value is extracted from the _s3-secret_ secret and added to the container defintion as the _s3_access_key_ environment variable.
====

==== Node Selectors, Taints and Tolerations

Selecting the correct worker node to execute a pipeline step is an important part of pipeline development. Specific nodes may have dedicated hardware such as GPUs; or there may be other constraints such as data locality. 

In our example we're using the nodes with an attached GPU to execute the step. To do this we need to:


. Create the requisite toleration:

include::example$sample-pipeline-full.py[lines=464..467]

. Add the _toleration_ to the pod and add a _node selector_ constraint.

include::example$sample-pipeline-full.py[lines=477..480]


[TIP]
====
You could also use this approach to ensure that pods without GPU needs are *not* scheduled to nodes with GPUs.

For global pipeline pod settings take a look at the *_PipelineConf_* class in the 'https://kubeflow-pipelines.readthedocs.io/en/1.8.22/source/kfp.dsl.html?highlight=add_env_variable#kfp.dsl.PipelineConf'[KFP SDK Documentation]. 
====


[NOTE]
====
We have only covered a _subset_ of what's possible with the _KFP SDK_.

It is also possible to customise significant parts of the _pod spec_ definition with:

* Init and Sidecar Pods
* Pod affinity rules
* Annotations and labels
* Retries and Timeouts
* Resource requests and limits

See the the https://kubeflow-pipelines.readthedocs.io/en/1.8.22/source/kfp.dsl.html[KFP SDK Documentation] for more details.
====

=== Compiling to Tekton 

As stated previously, DSP Python scripts need to be compiled into Tekton definitions for execution. 
This can be achieved in multiple ways:

. Explicity calling the "dsl-compile" command from the _kfp_ Python package giving the input and output files, then uploading the resultant yaml file to the DSP server via the UI.
. Adding the compile step to the Python script and then uploading the resultant yaml file to the DSP server via the UI.
. Adding the compile step to the Python script and uploading the resulting Tekton definition via an api call.

In our example we've chosen the second option:

include::example$sample-pipeline-full.py[lines=511]


The Tekton compiler also has a number of _global settings_, which are not covered here, see https://github.com/kubeflow/kfp-tekton/blob/master/sdk/python/kfp_tekton/compiler/pipeline_utils.py[here] for more details.


[IMPORTANT]
====
We have only covered a subset of the functionality avaiable in DSP as it pertains to our real-life scenario. Please see the https://github.com/kubeflow/kfp-tekton/blob/master/sdk/FEATURES.md[kfp-tekton features] document for more advanced functionality. The https://github.com/kubeflow/kfp-tekton/blob/master/guides/advanced_user_guide.md[KFP Tekton Advanced User Guide] also has more information.
====

=== Pipeline Execution

==== Submitting a Pipeline and Triggering a run

The following code demonstrates how to submit and trigger a pipeline run from a _Red Hat OpenShift AI WorkBench_.

[source, python]
if __name__ == '__main__':  
    kubeflow_endpoint = 'http://ds-pipeline-pipelines-definition:8888'
    sa_token_file_path = '/var/run/secrets/kubernetes.io/serviceaccount/token'
    with open(sa_token_file_path, 'r') as token_file:
        bearer_token = token_file.read()
    print(f'Connecting to Data Science Pipelines: {kubeflow_endpoint}')
    client = TektonClient(
        host=kubeflow_endpoint,
        existing_token=bearer_token
    )
    result = client.create_run_from_pipeline_func(
        offline_scoring_pipeline,
        arguments={},
        experiment_name='offline-scoring-kfp'
    )

==== Externally Triggering a DSP pipeline run

In our real-world example above the entire pipeline is executed when a file is added to an S3 bucket. Here is the process followed:

. File added to S3 bucket.
. S3 triggers the send of a webhook payload to an _OCP Serverless_ function.
. The _Serverless_ function parses the payload and invokes the configured _DSP pipeline_.

We're not going to go through the code and configuration for this, but here is the code to trigger the pipeline.

[source,python]
include::example$dsp_trigger.py[lines=34..51]


The full code is xref:attachment$dsp_trigger.py[here].

[NOTE]
====
The _pipeline_ needs to have already been submitted to the DSP runtime.
====


== Data Handling in Data Science Pipelines
DSP have two sizes of data, conviently named *_Small Data_* and *_Big Data_*.

. _Small Data_ is considered anything that can be passed as a _command line argument_ for example _Strings_, _URLS_, _Numbers_. The overall size should not exceed a few _kilobytes_.

. Unsurprisingly, everything else is considered _Big Data_ and should be passed as files.

=== Handling large data sets

DSP support two methods by which to pass large data sets aka _Big Data_ between pipeline steps:

. *_Tekton Workspaces_*.
. *_Volume based data passing method_*.

[NOTE]
====
The Data Science Projects *_Data Connection_* S3 storage is used to store _Output Artifacts_ and _Parameters_ of the stages of a pipeline. It is not intended to be used to pass large amounts of data between pipeline steps.
====

=== Tekton Workspaces

This uses the native Tekton mechanism for storing and passing data between stages of a _Tekton Pipeline_.
Tekton creates a *Workspace* to share large data files among tasks that run in the same pipeline. These _Workspaces_ are backed by dynamically created storage volumes which are removed once the pipeline has completed.

By default the storage is set to *_ReadWriteMany_*, the size is set to *_2Gi_* and uses the storage class called *_kfp-csi-s3_*. 

However, this can be changed to suit the target environment needs by setting the following *_Environment Variables_*:

. _DEFAULT_ACCESSMODES_
. _DEFAULT_STORAGE_SIZE_
. _DEFAULT_STORAGE_CLASS_

An example of this in the _Sample Pipeline_ is shown below:  

[source,python]
include::example$sample-pipeline-full.py[lines=503..505]

=== Volume-based data passing method
This approach uses a pre-created OpenShift storage volume (aka _PVC_) to pass data between the pipeline steps.
An example of this is in the https://github.com/kubeflow/kfp-tekton/blob/master/sdk/python/tests/compiler/testdata/artifact_passing_using_volume.py[KFP compiler tests] which we will discuss here.

First create the volume to be used and assign it to a variable:
[source,python]
include::example$artifact_passing_using_volume.py[lines=78..79]

[source,python]
include::example$artifact_passing_using_volume.py[lines=81..88]

Then add definition to the _pipeline configuration_:
[source,python]
include::example$artifact_passing_using_volume.py[lines=91..93]


[IMPORTANT]
====
The *_data-volume PVC claim_* needs to exist in the OpenShift namespace while running the pipeline, else the _pipeline execution pod_ fails to deploy and the run terminates.
====

To pass big data using cloud provider volumes, it's recommended to use the *_volume-based data passing method_*.

== Data Science Pipeline - Hands On Example

We're now going to deploy and run a sample pipeline. The use case is identifying fraudulent transactions among a large number of credit card transactions using an offline scoring approach.

=== Setup

[%interactive]
* [ ] Using the Minio UI, create a new S3 Bucket named _fraud_detection_bucket_.

image::create-s3-fraud-bucket.png[]

[%interactive]
* [ ] Download the following artifacts:

** [ ] xref:attachment$model-latest.onnx[ONNX Model]
** [ ] xref:attachment$live-data.csv.gz[Live Transaction Data]

[%interactive]
* [ ] Uncompress the *_.gz_* files using the `+gzip -d+` command. 
* [ ] Upload the files to S3 _fraud_detection_bucket_ via the _Minio_ UI.

====
image::fraud-upload-files.png[]
====
====
image::fraud-files-uploaded.png[]
====

[%interactive]
* [ ] Create the *_aws-connection-fraud-detection_* _secret_ containing the _S3_ bucket name, endpoint and credentials.

[source,bash]
--
oc create secret generic aws-connection-fraud-detection --from-literal=AWS_ACCESS_KEY_ID=minio --from-literal=AWS_DEFAULT_REGION=rhods-enablement-emea --from-literal=AWS_S3_BUCKET=fraud-detection-bucket --from-literal=AWS_S3_ENDPOINT=https://minio-api... --from-literal=AWS_SECRET_ACCESS_KEY=minio123
--

[%interactive]
* [ ] Create two OpenShift Persistent Volumes with the following names:
** *offline-scoring-model-volume*
** *offline-scoring-data-volume*

TIP: Example PVC definitions are here, download and customise as necessary this xref:attachment$offline-scoring-data-pvc.yaml[Data Volume definition] & xref:attachment$offline-scoring-model-pvc.yaml[Model Volume definition]


=== Execute Pipeline

Download and customise as necessary this xref:attachment$offline_scoring_kfp_pipeline.py[Example Pipeline definition]

[%interactive]
* [ ] Create a _Workbench_ with the name _fraud-onnx_

image::fraud-onnx-workbench.png[]

[%interactive]
* [ ] Download the xref:attachment$offline_scoring_kfp_pipeline.py[Offline Scoring Pipeline] and load it into your _workbench_.
* [ ] Click 'Restart Kernel and Run All Cells'.

image::start-pipeline.png[]

The Pipeline will be submitted to the server and a run _Triggered_.

image::pipeline-run-triggered.png[]

If the pipeline run was executed without any issues, you should see something similar to below:

image::pipeline-finish.png[]

=== Handling Pipeline Errors

In case the pipeline fails, the UI shows something similar to the following:

image::pipeline-failure.png[]

Clicking on the *_failed_* node in the graph and then clicking on the _Details_ tab presents the status of the execution step. 
When the mouse is placed over the *_Status_* field, the command to retrieve the detailed logs of the execution step is displayed.

image::pipeline-error-details.png[]

The step logs can then be viewed using the command provided.

image::pipeline-pod-error.png[]
