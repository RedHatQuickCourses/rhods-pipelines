= Section 2

== Data Science Pipelines in OpenShift AI

== Data Science Pipeline Concepts

* *Pipeline* -  is a workflow definition containing the steps and their input and output artifacts.
* *Step* - is a self-contained pipeline component that represents an execution stage in the pipeline.
* *Experiment* - is a logical grouping of runs for the purpose of comparing different pipeline configurations, for instance.
* *Run* - is a _single_ execution of a pipeline whereas a _recurring run_ is a scheduled, repeated execution of a pipeline.

[NOTE]
====
Pipleines are a basically an execution graph of tasks, this graph is commonly known as a _DAG_ (Directed Acyclic Graph).
A DAG is a directed graph with no cycles aka no direct loops.
====

OpenShift AI offers two out of the box mechanisms to build Data Science Pipelines.

The first mechanism is the Elyra Jupyter notebook addon which provides a visual editor for creating pipelines via orchestration of Jupyter notebooks. 

The second mechanism and the one discussed here is using Data Science Pipelines (DSP) based on the upstream KubeflowPipelines V1. With DSP, pipelines are built using python scripts using the KFPV1 python SDK. Once built, the python pipeline is submitted to the DSP runtime to be scheduled for execution.

[NOTE]
====
There is work in progress to migrate DSP to Kubeflow Pipeline V2. At the time of writing this is still a work in progress and we won't cover it here.
====

In OpenShift AI we use the *_Tekton_* runtime to execute pipelines. With this there is an additional step to be performed the python script needs to be compiled into a Tekton definition before being submitting to the runtime

In OpenShift AI the Data Science Pipeline runtime consists of the following components:

* A Data Science Pipeline Server container. 
* A MariaDB for storing pipeline definitions and results.
* A Pipeline scheduler for scheduling pipeline runs.
* A Persistent Agent to record the set of containers that executed as well as their inputs and outputs.

Steps in the pipeline are executed as ephemeral pods (one per step).

[NOTE]
====
DS Pipeline in OpenShift AI are managed by the `data-science-pipelines-operator-controller-manager` operator in the `redhat-ods-applications` namespace. The CRD is an instance of _datasciencepipelinesapplications.datasciencepipelinesapplications.opendatahub.io_
====

.DataScience Pipeline pods
[source,cmd]
----
 ~/oc get pods -n redhat-ods-applications
NAME                                                              READY   STATUS    RESTARTS   AGE
ds-pipeline-persistenceagent-pipelines-definition-6d6676d58vzss   1/1     Running   0          9m49s
ds-pipeline-pipelines-definition-7c7dd56b4d-qfz8t                 2/2     Running   0          9m49s
ds-pipeline-scheduledworkflow-pipelines-definition-7f5c645hcg4g   1/1     Running   0          9m48s
mariadb-pipelines-definition-795c57795b-27ljh                     1/1     Running   0          9m49s
----

== Creating a Data Science Pipeline with the KFP SDK

=== Prerequisites 
* Python Setup
[source,cmd]
----
pip install kfp==1.8
pip install kfp-tekton==1.7.2
----
IMPORTANT: Using the correct python module versions is critical to avoid conflicts between kfp SDK versions

* S3 Storage

Follow these instructions to setup a local Minio instance
https://ai-on-openshift.io/tools-and-applications/minio/minio/


=== Creating a Pipeline Server

To execute pipelines a _Pipeline server_ needs to be created but before that can happen an S3 Storage bucket needs to be configured. This is used to store the run artifacts and outputs of any pipeline run on the associated server. 
****
image::creating-data-connection.png[]
****
Once the data connection is created then the pipeline server can be created
****
image::pre-creating-pipeline-server.png[]
****
****
image::pipeline-server-created.png[]
****
=== Building and deploying a Pipeline

The following is a simplistic example of a KFP pipeline. The original is at 
https://github.com/kubeflow/kfp-tekton/blob/master/samples/flip-coin/condition.py and the example has downloaded it to a file named _coin-toss.py_

****
.Python Pipeline Example
[source,python]
----
# Copyright 2020 kubeflow.org
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from kfp import dsl
from kfp import components

def random_num(low:int, high:int) -> int:
    """Generate a random number between low and high."""
    import random
    result = random.randint(low, high)
    print(result)
    return result

def flip_coin() -> str:
    """Flip a coin and output heads or tails randomly."""
    import random
    result = 'heads' if random.randint(0, 1) == 0 else 'tails'
    print(result)
    return result

def print_msg(msg: str):
    """Print a message."""
    print(msg)


flip_coin_op = components.create_component_from_func(
    flip_coin, base_image='python:alpine3.6')
print_op = components.create_component_from_func(
    print_msg, base_image='python:alpine3.6')
random_num_op = components.create_component_from_func(
    random_num, base_image='python:alpine3.6')

@dsl.pipeline(
    name='conditional-execution-pipeline',
    description='Shows how to use dsl.Condition().'
)
def flipcoin_pipeline():
    flip = flip_coin_op()
    with dsl.Condition(flip.output == 'heads'):
        random_num_head = random_num_op(0, 9)
        with dsl.Condition(random_num_head.output > 5):
            print_op('heads and %s > 5!' % random_num_head.output)
        with dsl.Condition(random_num_head.output <= 5):
            print_op('heads and %s <= 5!' % random_num_head.output)

    with dsl.Condition(flip.output == 'tails'):
        random_num_tail = random_num_op(10, 19)
        with dsl.Condition(random_num_tail.output > 15):
            print_op('tails and %s > 15!' % random_num_tail.output)
        with dsl.Condition(random_num_tail.output <= 15):
            print_op('tails and %s <= 15!' % random_num_tail.output)


if __name__ == '__main__':
    from kfp_tekton.compiler import TektonCompiler
    TektonCompiler().compile(flipcoin_pipeline, __file__.replace('.py', '.yaml'))
----
****

To compile it into a Tekton resource definition just run the following in a terminal
[source,python]
----
python3 coin-toss.py
----

It will generate a Tekton *_Pipeline Run_* , similar to this snippet
****
[source,yaml]
----
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: conditional-execution-pipeline
  annotations:
    tekton.dev/output_artifacts: '{"flip-coin": [{"key": "artifacts/$PIPELINERUN/flip-coin/Output.tgz",
      "name": "flip-coin-Output", "path": "/tmp/outputs/Output/data"}], "random-num":
      [{"key": "artifacts/$PIPELINERUN/random-num/Output.tgz", "name": "random-num-Output",
      "path": "/tmp/outputs/Output/data"}], "random-num-2": [{"key": "artifacts/$PIPELINERUN/random-num-2/Output.tgz",
      "name": "random-num-2-Output", "path": "/tmp/outputs/Output/data"}]}'
    tekton.dev/input_artifacts: '{"print-msg": [{"name": "random-num-Output", "parent_task":
      "random-num"}], "print-msg-2": [{"name": "random-num-Output", "parent_task":
      "random-num"}], "print-msg-3": [{"name": "random-num-2-Output", "parent_task":
      "random-num-2"}], "print-msg-4": [{"name": "random-num-2-Output", "parent_task":
      "random-num-2"}]}'
    tekton.dev/artifact_bucket: mlpipeline
    tekton.dev/artifact_endpoint: minio-service.kubeflow:9000
    tekton.dev/artifact_endpoint_scheme: http://
    tekton.dev/artifact_items: '{"flip-coin": [["Output", "$(results.Output.path)"]],
      "print-msg": [], "print-msg-2": [], "print-msg-3": [], "print-msg-4": [], "random-num":
      [["Output", "$(results.Output.path)"]], "random-num-2": [["Output", "$(results.Output.path)"]]}'
    sidecar.istio.io/inject: "false"
    tekton.dev/template: ''
----
****

The resulting yaml file _(coin-toss.yaml)_ can then be uploaded throught the UI
****
image::import-pipeline.png[]
****
****
image::pipeline-imported.png[]
****
Once imported the structure of the _DAG_ will be shown. Each step in the pipeline will be run as a container on OpenShift.

****
image::pipeline-run-view.png[]
****

To execute the pipeline, click on _Create Run_ in the menu and fill out the _Name_ and _Description_.
If the pipeline has _Input Parameters_ or a you need to schedule a recurring run then that can be configured further down. Once ready clink _Create_ and the pipeline will be scheduled.

****
image::creating-pipeline-run.png[]
****

The pipeline will execute and the outputs will be stored into the configured S3 bucket.
As the pipeline executes the view will be updated to show the steps being executed. It's possible to click on the graph nodes to reveal information of the steps

****
image::post-pipeline-run.png[]
****

Once the pipeline has completed it is possible to access the output and pipeline artifacts (if used) in the Minio Storage UI

****
image::object-store-after-run.png[]
****

=== Experiments And Runs

An experiment is a workspace where you can try different configurations of your pipelines. You can use experiments to organize your runs into logical groups. Experiments can contain arbitrary runs, including recurring runs. 

A run can be configured using the DSP UI or programatically using the _kfp SDK_.

image::dsp-runs.png[]

NOTE: Experiments are part of the KFP SDK and is not currently covered in this course.

== Real World Example

In this section we're going to demonstrate a real world Data Science Pipelines scenario. 

****
In this scenario we have a remote edge device which uses an AI model to manage the characteristics of its battery usage depending on the environment it's deployed in. On a regular schedule it uploads battery events via a data gateway and those batter events are used to train a model which is then retrieved by the device and used. 


image::openshift-ai-dsp-edge.png[]
****

The entire pipeline is here xref:attachment$sample-pipeline-full.py[Pipeline]

We're not going to go through all of it but focus on the key aspects of it. 

The actual pipeline is defined by the following function:

****
[source,python]
include::example$sample-pipeline-full.py[lines=445..454]
****

The '@dsl.pipeline' parameters provide the name and description if you were uploading the pipeline via an API call. The DSP UI overwrites these values.

The function *_edgetest_pipeline_* is the implementation of the pipeline.

=== Pipeline Parameters
The pipeline has four parameters

* _file_obj_ and _src_bucket_ refer to S3 bucket details and can be ignored.
* _VIN_ is the edge device identifier and has a default value of 412356
* _epoch_count_ is the number of training epochs to be used

In the _Create Run_ UI these parameters are available so that users can override the values as they need

****
image::pipeline-parameters.png[]
****

=== Pipeline Steps
The file contains the following python functions which roughly correspond to the steps in the diagram above

* load_trigger_data()
* prep_data_train_model()
* model_upload_notify()
* model_inference()

These functions are mapped into individual containers by using the _create_component_from_func_ function. You can specify the container _base_image_ to use as well as any additional python packages to be installed into the container at execution time.

[source,python]
include::example$sample-pipeline-full.py[lines=433..443]

The python functions can be used in multiple different step definitions; in the example the _prep_data_train_model_ function is used in the _prep_data_train_op_ and the _prep_inference_data_op_ containers.

The pipeline execution _graph_ is created using the following code:

[source,python]
include::example$sample-pipeline-full.py[lines=471]

[source,python]
include::example$sample-pipeline-full.py[lines=477]

[source,python]
include::example$sample-pipeline-full.py[lines=482]

[source,python]
include::example$sample-pipeline-full.py[lines=492]

[source,python]
include::example$sample-pipeline-full.py[lines=495]


[IMPORTANT] 
====
The execution order of the graph is top down but also can be controlled by using the *_.after()_* operator. 

There are other operators which control the flow of pipeline execution such as _Condition_ , _ExitHandler_, _ParallelFor_ .
These are not covered as part of this course but the the 'https://github.com/kubeflow/pipelines/blob/master/samples/tutorials/DSL%20-%20Control%20structures/DSL%20-%20Control%20structures.py[KFP documentation]' has examples.
====

The following diagram shows the order of execution.

****
image::pipeline-graph.png[]
****

This is also visable in the OpenShift AI user interface:

****
image::oai-pipeline-graph.png[]
****

=== Pipeline Parameter Passing
As each step of our pipeline is executed in an independent container, input parameters and output values are handled as follows.

==== Input Parameters

* Simple parameters - booleans, numbers, strings - are passed by value into the container as command line arguments.
* Complex types or large amounts of data are passed via files. The value of the input parameter is the file path.

==== Output Parameters

* Output values are returned via files.

==== Passing Parameters via Files
To pass an input parameter as a file, the function argument needs to be annotated using the _InputPath_ annotation.
For returning data from a step as a file, the function argument needs to be annotated using the _OutputPath_ annotation.

In both cases the actual value of the parameter is the file path and not the actual data. So the pipeline will have to read/write to the file as necessary.

For example in our sample pipeline we use the _parameter_data_ argument of the _prep_data_train_model_ function to return multiple data values as a file, here's the function definition with the _OutputPath_ annotation

[source,python]
include::example$sample-pipeline-full.py[lines=51]

Here's the actual writing of the data to the file

[source,python]
include::example$sample-pipeline-full.py[lines=238..242]

This data is then consumed in the _model_upload_notify_ function, passed via the _paramater_data_ with the _InputPath_ annotation.

[source,python]
include::example$sample-pipeline-full.py[lines=243]

Reading the data

[source,python]
include::example$sample-pipeline-full.py[lines=275..276]


Linking the two functions together 

[source,python]
include::example$sample-pipeline-full.py[lines=482]


[TIP]
====
There are other parameter annotations available to handle specialised file types 
For example _InputBinaryFile_, _OutputBinaryFile_. 

The full annotation list is in the https://kubeflow-pipelines.readthedocs.io/en/1.8.22/source/kfp.components.html[KFP component documentation].

====

==== Returning multiple values from a step 
If you return a single small value from your component using the _return_ statement, the output parameter is named *_output_*.
It is however possible to return multiple small values using the python _collection_ library method _namedtuple_

From a https://github.com/kubeflow/pipelines/blob/master/samples/tutorials/Data%20passing%20in%20python%20components.ipynb[Kubeflow pipelines example]
 
[source,python]
----
def produce_two_small_outputs() -> NamedTuple('Outputs', [('text', str), ('number', int)]):
    return ("data 1", 42)
consume_task3 = consume_two_arguments(produce2_task.outputs['text'], produce2_task.outputs['number'])
----

====
The kfp SDK uses the following rules to define the input and output parameter names in your component’s interface:

    . If the argument name ends with _path and the argument is annotated as an _kfp.components.InputPath_ or _kfp.components.OutputPath_, the parameter name is the argument name with the trailing _path removed.
    . If the argument name ends with _file, the parameter name is the argument name with the trailing _file removed.
    . If you return a single small value from your component using the return statement, the output parameter is named *output*.
    . If you return several small values from your component by returning a _collections.namedtuple_, the SDK uses the tuple’s field names as the output parameter names.

    . Otherwise, the SDK uses the argument name as the parameter name.
====

[TIP]
====
In the Tekton definition you can see the definition of the _input and output artifacts_ this can be useful for debugging purposes

include::example$sample-pipeline-full.yaml[lines=12..18]

include::example$sample-pipeline-full.yaml[lines=6..11]

include::example$sample-pipeline-full.yaml[lines=22..25]

You can also see the locations of data stored into the _s3_ bucket e.g. _artifacts/$PIPELINERUN/prep-data-train-model-2/parameter_data.tgz_
====

=== Execution on OpenShift

To enable the _pipeline_ to run on OpenShift we need to pass it the associated _kubernetes_ resources 

* _volumes_ 
* _environment variables_
* _node selectors, taints and tolerations_

==== Volumes
Our pipeline requires a number of volumes to be created and mounted into the executing pods. The volumes are primarily used for storage and secrets handling but can also be used for passing configuration files into the pods.

Before mounting the volumes into the pods they need to be created. The following code creates two volumes, one from a pre-existing PVC and another from a pre-existing secret.

include::example$sample-pipeline-full.py[lines=453..462]

The volumes are mounted into the containers using the *_add_pvolumes_* method:

include::example$sample-pipeline-full.py[lines=495..497]

==== Environment Variables

Environment variables can be added to the pod using the *_add_env_variable_* method. 

include::example$sample-pipeline-full.py[lines=471..475]

[NOTE]
====
The *_env_from_secret_* utility method also enables extracting values from secrets and mounting them as environment variables, in the example above the _AWS_ACCESS_KEY_ID_ value is extracted from the _s3-secret_ secret and added to the container defintion as the _s3_access_key_ environment variable.
====

==== Node Selectors, Taints and Tolerations

Selecting the correct worker node to execute a pipeline step is an important part of pipeline development. Specific nodes may have dedicated hardware i.e. GPUs etc, also there may be other constraints e.g. data locality. 

In our example we're using the nodes with an attached GPU to execute the step, to do this we need to:


. Create the requisite toleration:

include::example$sample-pipeline-full.py[lines=464..467]

. Add the _toleration_ to the pod and add a _node selector_ constraint.

include::example$sample-pipeline-full.py[lines=477..480]


[TIP]
====
You could also use this approach to ensure that pods without GPU needs are *not* scheduled to nodes with GPUs.

For global pipeline pod settings take a look at the *_PipelineConf_* class in the 'https://kubeflow-pipelines.readthedocs.io/en/1.8.22/source/kfp.dsl.html?highlight=add_env_variable#kfp.dsl.PipelineConf'[KFP SDK Documentation]. 
====


[NOTE]
====
We have only covered a _subset_ of what's possible with the _kfp SDK_.

It is also possible to customise significant parts of the _pod spec_ definition with:

* Init and Sidecar Pods
* Pod affinity rules
* Annotations and labels
* Retries and Timeouts
* Resource requests and limits

See the the https://kubeflow-pipelines.readthedocs.io/en/1.8.22/source/kfp.dsl.html[KFP SDK Documentation] for more details.
====

=== Compiling to Tekton 

As stated previously DSP python scripts need to be compiled into Tekton definitions for execution. 
This can be achieved in multiple ways:

. Explicity calling the "dsl-compile" command from the _kfp_ python package giving the input and output files. Then uploading the resultant yaml file to the DSP server via the UI
. Adding the compile step to the python script and then uploading the resultant yaml file to the DSP server via the UI.
. Adding the compile step to the python script and uploading the resulting tekton definition via an api call.

In our example we've chosen the second option for no good particular reason:

include::example$sample-pipeline-full.py[lines=511]


The Tekton compiler also has a number of _global setting_ which are not covered here see https://github.com/kubeflow/kfp-tekton/blob/master/sdk/python/kfp_tekton/compiler/pipeline_utils.py[Here] for more.


[IMPORTANT]
====
We have only covered a subset of the functionality avaiable in DSP as it pertains to our real-life scenario. Please see the https://github.com/kubeflow/kfp-tekton/blob/master/sdk/FEATURES.md[kfp-tekton features] document for more advanced functionality.
====

=== Triggering a DSP pipeline

In our real-world example above the entire pipeline is executed when a file is added to an _s3_ bucket. Here is the process followed:

. File added to _s3_ bucket.
. S3 triggers the send of a webhook payload to a _OCP Serverless_ function
. The _Serverless_ function parses the payload and invokes the configured _DSP pipeline_

We're not going to go through all of the code and configuration for this but here is the code to trigger the pipeline.

[source,python]
include::example$dsp_trigger.py[lines=34..51]


The full code is xref:attachment$dsp_trigger.py[Here]